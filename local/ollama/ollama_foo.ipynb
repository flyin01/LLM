{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360370ee",
   "metadata": {},
   "source": [
    "## Ollama local LLMs  \n",
    "\n",
    "This notebook is using Ollama and the following for running local LLMs:  \n",
    "* ollama sdk\n",
    "* openai sdk \n",
    "  \n",
    "Source 1: [Ollama Docs](https://docs.ollama.com/capabilities/tool-calling#python-2)  \n",
    "Source 2: [OpenaAI Cookbook](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)  \n",
    "  \n",
    "**Setup:**  \n",
    "1. To load a local model run the following from the terminal: `ollama run gpt-oss:20b`\n",
    "2. Run the notebook using venv and install libs from `requirements.txt`\n",
    "  \n",
    "### 1. Ollama SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9882de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\repo\\LLM\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b82d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '/requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r /requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9985ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45e8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa1bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Propose the optimal prompt for the gpt-oss:20b model.\n",
      "        Given an example of a poorly constructed prompt and improve it until it is optiomal.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model\n",
    "message = \"\"\"\n",
    "        Propose the optimal prompt for the gpt-oss:20b model.\n",
    "        Given an example of a poorly constructed prompt and improve it until it is optiomal.\n",
    "        \"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427369d5",
   "metadata": {},
   "source": [
    "#### Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "519cf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: \n",
      " We need to propose optimal prompt for GPT-OSS:20B model. Provide an example of a poorly constructed prompt and improve it until optimal. Likely we need to discuss aspects: context, clarity, length, format, explicitness, etc. Provide before and after examples. Also propose the final prompt guidelines. So answer: first a general best-practice prompt. Then an example of bad prompt, improvement steps.\n",
      "\n",
      "We should keep it succinct but thorough. Provide guidelines: specify role, task, constraints, format, etc.\n",
      "\n",
      "We need to mention that GPT-OSS:20B is an open-source large language model; likely similar to GPT-3/4. Provide prompt design guidelines: role specification, instruction clarity, context, examples, formatting, output specification, etc. Provide sample prompts: initial poorly constructed (e.g., \"Explain quantum computing.\") and improved version: \"You are a quantum physicist... produce a 300-word explanation...\".\n",
      "\n",
      "But the user says: \"Propose the optimal prompt for the gpt-oss:20b model. Given an example of a poorly constructed prompt and improve it until it is optimal.\"\n",
      "\n",
      "Thus we propose optimal prompt template and show improvement.\n",
      "\n",
      "So answer: 1. General optimal prompt template. 2. Example of poor prompt. 3. Step-by-step improvement.\n",
      "\n",
      "We can also mention use of \"system\" vs \"user\" role, but GPT-OSS may not have system role? Typically open-source models don't have system role but can be given as context. So we can embed \"You are...\" in prompt.\n",
      "\n",
      "Also specify token count, avoid ambiguous instructions, use explicit formatting like bullet points.\n",
      "\n",
      "Let's produce. \n",
      "\n",
      "Answer: \n",
      " ## 1.  “Optimal Prompt” for **gpt‑oss:20B**\n",
      "\n",
      "> **Purpose** – The prompt is the *only* thing you can hand to a large‑language‑model (LLM) to shape its behavior.  \n",
      "> **Goal** – Make the LLM produce a single, high‑quality answer that follows your constraints exactly, using the fewest possible tokens so the model has more capacity for the answer itself.\n",
      "\n",
      "Below is a **template** you can copy‑paste and adjust for almost any task.  It contains all the ingredients that research and practice have shown to work best with a 20‑B‑parameter LLM:\n",
      "\n",
      "```text\n",
      "You are a {role: e.g. \"professional copywriter\"} who {has expertise in X: e.g. \"B2B SaaS marketing\"}.\n",
      "\n",
      "Task: {explicit, concise instruction}\n",
      "    • {Primary objective: e.g. \"Write a 150‑word product description\"}\n",
      "    • {Tone: e.g. \"Professional and persuasive\"}\n",
      "    • {Target audience: e.g. \"C‑level executives in the tech industry\"}\n",
      "    • {Formatting: e.g. \"Start with a headline, then a short paragraph\"].\n",
      "\n",
      "Constraints: \n",
      "    • No personal data or private info\n",
      "    • Avoid filler phrases like “in my opinion”\n",
      "    • Use only the following keywords: {list of allowed words or themes}\n",
      "    • Do not exceed {max_length} characters\n",
      "\n",
      "Deliverables: \n",
      "    1. The complete text only (no explanatory notes)\n",
      "    2. End with a single “END” marker\n",
      "\n",
      "Example (optional, only if the task is complex):\n",
      "```\n",
      "\n",
      "> **Why this works**  \n",
      "> * **Role + Expertise**: Positions the model in the right “world view.”  \n",
      "> * **Task + Sub‑tasks**: Splits a big ask into bite‑sized pieces the model can follow.  \n",
      "> * **Constraints**: Forces compliance and removes unwanted content.  \n",
      "> * **Deliverables**: Tells the model exactly what to output (no extra chatter).  \n",
      "> * **Example**: Helps the model “ground” its answer, especially for nuanced or domain‑specific tasks.\n",
      "\n",
      "Feel free to drop or add items (e.g., a “Do’s & Don’ts” list, a “Tone” section, etc.) as the task demands.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Example of a Poor Prompt and Its Progressive Refinements\n",
      "\n",
      "| Stage | Prompt | Issues | How to Fix |\n",
      "|-------|--------|--------|------------|\n",
      "| **Initial** | `\"Explain quantum computing.\"` | • *Vague*: No context, length, tone, or format specified.<br>• *Ambiguous*: The model may give a very short definition or a very long essay.<br>• *No constraints*: Could inadvertently produce copyrighted text. | Add role, length, tone, and a request for a specific structure. |\n",
      "| **Step 1** | `\"Explain quantum computing in plain language for beginners, 200 words.\"` | • Still vague on format (paragraphs, bullet points).<br>• No tone specified (e.g., friendly vs. formal). | Clarify formatting and tone. |\n",
      "| **Step 2** | `\"You are a science educator. Write a 200‑word explanation of quantum computing for high‑school students. Use bullet points for key concepts and a friendly tone.\"` | • Great, but still missing explicit constraints (e.g., no jargon, no copyrighted terms).<br>• No deliverable marker. | Add constraints and output instructions. |\n",
      "| **Step 3** | `\"You are a science educator. Write a 200‑word, bullet‑pointed explanation of quantum computing for high‑school students in a friendly tone. Avoid jargon, use only words from the following list: [quantum, bits, entanglement, superposition, algorithm, computer]. Do not exceed 1200 characters. End with 'END'.\"` | • Now the prompt is highly specific, but it might still produce extra commentary (\"Here's how I did it\") if the model interprets the last sentence as an instruction rather than a marker. | Add a clear deliverable section to suppress any meta‑commentary. |\n",
      "| **Optimal** | ```text\n",
      "You are a science educator with 10+ years teaching high‑school physics.\n",
      "\n",
      "Task:  \n",
      "• Write a 200‑word, bullet‑pointed explanation of quantum computing.  \n",
      "• Target audience: high‑school students.  \n",
      "• Tone: friendly, encouraging.  \n",
      "\n",
      "Constraints:  \n",
      "• Use only these words: quantum, bits, entanglement, superposition, algorithm, computer.  \n",
      "• No jargon or abbreviations.  \n",
      "• Do not exceed 1200 characters.  \n",
      "\n",
      "Deliverables:  \n",
      "1. The bullet‑pointed text only.  \n",
      "2. End the response with a single word: END.  \n",
      "``` | • All essential information is present, no ambiguity.  \n",
      "• The LLM now has a *clear checklist* it can check against while generating.  \n",
      "• Length & character limits keep the answer tight.  \n",
      "• The explicit “END” marker guarantees the model does not add post‑script explanations. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Quick Checklist for Crafting Your Own Optimal Prompt\n",
      "\n",
      "| What to include | Why it matters |\n",
      "|-----------------|----------------|\n",
      "| **Role + Expertise** | Grounds the LLM in a worldview that aligns with your needs. |\n",
      "| **Explicit Task** (with sub‑tasks) | Prevents mis‑interpretation of “explain” vs “summarize” vs “compare.” |\n",
      "| **Tone / Audience** | Controls emotional register and complexity level. |\n",
      "| **Formatting Requirements** | Forces output to match UI or downstream requirements. |\n",
      "| **Content Constraints** (keyword list, style rules, prohibited phrases) | Keeps the model compliant with policy or brand voice. |\n",
      "| **Length / Character Limit** | Conserves token budget for the answer itself. |\n",
      "| **Deliverables / End Marker** | Eliminates extraneous commentary and signals completion. |\n",
      "| **Optional Example** | Helps the model “anchor” on a concrete style, especially for creative or highly specialized tasks. |\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "- **Optimal Prompt** = *Role + Task + Constraints + Deliverables*, all written in a single, concise block of text.  \n",
      "- **Start with a vague prompt**, then **iteratively add** role, tone, format, constraints, and deliverable markers until the model can’t miss any instruction.  \n",
      "- **Never rely on the model to infer style or length**; always spell it out.  \n",
      "\n",
      "Use the template above as a one‑page cheat‑sheet, tweak the placeholders for your use case, and you’ll consistently get high‑quality, policy‑compliant answers from gpt‑oss:20B. Happy prompting!\n"
     ]
    }
   ],
   "source": [
    "# Define the chat interaction with the model\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=[{'role': 'user',\n",
    "               'content':message}],\n",
    "               think=True,\n",
    "               stream=False\n",
    ")\n",
    "\n",
    "print('Thinking: \\n', response.message.thinking, '\\n')\n",
    "print('Answer: \\n', response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a6483fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 1.  “Optimal Prompt” for **gpt‑oss:20B**\n",
      "\n",
      "> **Purpose** – The prompt is the *only* thing you can hand to a large‑language‑model (LLM) to shape its behavior.  \n",
      "> **Goal** – Make the LLM produce a single, high‑quality answer that follows your constraints exactly, using the fewest possible tokens so the model has more capacity for the answer itself.\n",
      "\n",
      "Below is a **template** you can copy‑paste and adjust for almost any task.  It contains all the ingredients that research and practice have shown to work best with a 20‑B‑parameter LLM:\n",
      "\n",
      "```text\n",
      "You are a {role: e.g. \"professional copywriter\"} who {has expertise in X: e.g. \"B2B SaaS marketing\"}.\n",
      "\n",
      "Task: {explicit, concise instruction}\n",
      "    • {Primary objective: e.g. \"Write a 150‑word product description\"}\n",
      "    • {Tone: e.g. \"Professional and persuasive\"}\n",
      "    • {Target audience: e.g. \"C‑level executives in the tech industry\"}\n",
      "    • {Formatting: e.g. \"Start with a headline, then a short paragraph\"].\n",
      "\n",
      "Constraints: \n",
      "    • No personal data or private info\n",
      "    • Avoid filler phrases like “in my opinion”\n",
      "    • Use only the following keywords: {list of allowed words or themes}\n",
      "    • Do not exceed {max_length} characters\n",
      "\n",
      "Deliverables: \n",
      "    1. The complete text only (no explanatory notes)\n",
      "    2. End with a single “END” marker\n",
      "\n",
      "Example (optional, only if the task is complex):\n",
      "```\n",
      "\n",
      "> **Why this works**  \n",
      "> * **Role + Expertise**: Positions the model in the right “world view.”  \n",
      "> * **Task + Sub‑tasks**: Splits a big ask into bite‑sized pieces the model can follow.  \n",
      "> * **Constraints**: Forces compliance and removes unwanted content.  \n",
      "> * **Deliverables**: Tells the model exactly what to output (no extra chatter).  \n",
      "> * **Example**: Helps the model “ground” its answer, especially for nuanced or domain‑specific tasks.\n",
      "\n",
      "Feel free to drop or add items (e.g., a “Do’s & Don’ts” list, a “Tone” section, etc.) as the task demands.\n",
      "\n",
      "---\n",
      "\n",
      "## 2.  Example of a Poor Prompt and Its Progressive Refinements\n",
      "\n",
      "| Stage | Prompt | Issues | How to Fix |\n",
      "|-------|--------|--------|------------|\n",
      "| **Initial** | `\"Explain quantum computing.\"` | • *Vague*: No context, length, tone, or format specified.<br>• *Ambiguous*: The model may give a very short definition or a very long essay.<br>• *No constraints*: Could inadvertently produce copyrighted text. | Add role, length, tone, and a request for a specific structure. |\n",
      "| **Step 1** | `\"Explain quantum computing in plain language for beginners, 200 words.\"` | • Still vague on format (paragraphs, bullet points).<br>• No tone specified (e.g., friendly vs. formal). | Clarify formatting and tone. |\n",
      "| **Step 2** | `\"You are a science educator. Write a 200‑word explanation of quantum computing for high‑school students. Use bullet points for key concepts and a friendly tone.\"` | • Great, but still missing explicit constraints (e.g., no jargon, no copyrighted terms).<br>• No deliverable marker. | Add constraints and output instructions. |\n",
      "| **Step 3** | `\"You are a science educator. Write a 200‑word, bullet‑pointed explanation of quantum computing for high‑school students in a friendly tone. Avoid jargon, use only words from the following list: [quantum, bits, entanglement, superposition, algorithm, computer]. Do not exceed 1200 characters. End with 'END'.\"` | • Now the prompt is highly specific, but it might still produce extra commentary (\"Here's how I did it\") if the model interprets the last sentence as an instruction rather than a marker. | Add a clear deliverable section to suppress any meta‑commentary. |\n",
      "| **Optimal** | ```text\n",
      "You are a science educator with 10+ years teaching high‑school physics.\n",
      "\n",
      "Task:  \n",
      "• Write a 200‑word, bullet‑pointed explanation of quantum computing.  \n",
      "• Target audience: high‑school students.  \n",
      "• Tone: friendly, encouraging.  \n",
      "\n",
      "Constraints:  \n",
      "• Use only these words: quantum, bits, entanglement, superposition, algorithm, computer.  \n",
      "• No jargon or abbreviations.  \n",
      "• Do not exceed 1200 characters.  \n",
      "\n",
      "Deliverables:  \n",
      "1. The bullet‑pointed text only.  \n",
      "2. End the response with a single word: END.  \n",
      "``` | • All essential information is present, no ambiguity.  \n",
      "• The LLM now has a *clear checklist* it can check against while generating.  \n",
      "• Length & character limits keep the answer tight.  \n",
      "• The explicit “END” marker guarantees the model does not add post‑script explanations. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3.  Quick Checklist for Crafting Your Own Optimal Prompt\n",
      "\n",
      "| What to include | Why it matters |\n",
      "|-----------------|----------------|\n",
      "| **Role + Expertise** | Grounds the LLM in a worldview that aligns with your needs. |\n",
      "| **Explicit Task** (with sub‑tasks) | Prevents mis‑interpretation of “explain” vs “summarize” vs “compare.” |\n",
      "| **Tone / Audience** | Controls emotional register and complexity level. |\n",
      "| **Formatting Requirements** | Forces output to match UI or downstream requirements. |\n",
      "| **Content Constraints** (keyword list, style rules, prohibited phrases) | Keeps the model compliant with policy or brand voice. |\n",
      "| **Length / Character Limit** | Conserves token budget for the answer itself. |\n",
      "| **Deliverables / End Marker** | Eliminates extraneous commentary and signals completion. |\n",
      "| **Optional Example** | Helps the model “anchor” on a concrete style, especially for creative or highly specialized tasks. |\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "- **Optimal Prompt** = *Role + Task + Constraints + Deliverables*, all written in a single, concise block of text.  \n",
      "- **Start with a vague prompt**, then **iteratively add** role, tone, format, constraints, and deliverable markers until the model can’t miss any instruction.  \n",
      "- **Never rely on the model to infer style or length**; always spell it out.  \n",
      "\n",
      "Use the template above as a one‑page cheat‑sheet, tweak the placeholders for your use case, and you’ll consistently get high‑quality, policy‑compliant answers from gpt‑oss:20B. Happy prompting!\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a436e",
   "metadata": {},
   "source": [
    "#### Tools calling alt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5532f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tool \n",
    "def square_nums(number: int) -> int:\n",
    "    \"\"\"This function returns the square of a number.\n",
    "    Args:\n",
    "        number (int): The number to be squared.\"\"\"\n",
    "    results = number * number\n",
    "    \n",
    "    return results\n",
    "\n",
    "square_nums(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0170d6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: \n",
      " The user asks: \"What is the square of 22?\" We should use the tool \"square_nums\" to compute. Then respond. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model and use the tool\n",
    "message = \"\"\"\n",
    "        What is the square of 22?\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user',\n",
    "     'content': message,\n",
    "    }\n",
    "]\n",
    "\n",
    "# pass tool to the model\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=messages,\n",
    "    tools=[square_nums],\n",
    "    think=True)\n",
    "\n",
    "print('Thinking: \\n', response.message.thinking, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# print('Answer: \\n', response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0389e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Thinking: \n",
      " None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Break down this function calling cell - TBD\n",
    "messages.append(response.message)\n",
    "if response.message.tool_calls:\n",
    "  # only recommended for models which only return a single tool call\n",
    "  call = response.message.tool_calls[0]\n",
    "  result = square_nums(**call.function.arguments)\n",
    "\n",
    "  # add the tool result to the messages\n",
    "  messages.append({\"role\": \"tool\", \n",
    "                   \"tool_name\": call.function.name, \n",
    "                   \"content\": str(result)})\n",
    "\n",
    "  final_response = chat(model=\"gpt-oss:20b\", \n",
    "                        messages=messages, \n",
    "                        tools=[square_nums], \n",
    "                        think=True)\n",
    "  \n",
    "  print('Final Thinking: \\n', final_response.message.thinking, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "375f9115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The square of 22 is **484**.\n"
     ]
    }
   ],
   "source": [
    "print(final_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4e987",
   "metadata": {},
   "source": [
    "### 2. OpenAI SDK\n",
    "\n",
    "### Tools calling alt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfb16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library version: 2.8.1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(\"Library version:\", openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ac82e",
   "metadata": {},
   "source": [
    "Ollama exposes a Chat Completion-compatible API, so we can use the OpenAI SDK withouth chaning much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d77b9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Prompt‑Engineering 101 for **GPT‑OSS:20B**\n",
      "\n",
      "*GPT‑OSS:20B* is a 20‑billion‑parameter open‑source transformer (≈ ≈ 10 GB of model weights).  \n",
      "It behaves similarly to GPT‑3.5‑turbo but has a **≈ 4 k–5 k token context window** (depending on the build) and no built‑in instruction‑following or safety filters. That means the *quality* of your prompt is the single biggest lever for consistent, useful results.\n",
      "\n",
      "Below is a practical checklist and set of templates you can copy‑paste, tweak, or build on.  \n",
      "\n",
      "---\n",
      "\n",
      "### 1. Understand the Basics\n",
      "\n",
      "| What you need to know | Why it matters |\n",
      "|-----------------------|----------------|\n",
      "| **Token limit** | 4 k–5 k tokens max. Keep the prompt + expected reply under that. |\n",
      "| **No implicit safety** | The model can hallucinate or produce harmful content. Use filtering or post‑processing. |\n",
      "| **No external memory** | Each generation is independent; you must pass all context you want the model to see. |\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Three‑Part Prompt Architecture\n",
      "\n",
      "| Section | Intent | Example |\n",
      "|---------|--------|---------|\n",
      "| **System prompt** | Sets the model’s *overall personality* and *tone*; it’s read *once* unless you reset. | `You are a helpful, concise assistant that always asks clarifying questions before answering.` |\n",
      "| **User prompt** | The actual user instruction + any data you’re handing the model. | `Summarize the following article (max 200 words) <article-text>` |\n",
      "| **Assistant prompt** | The model’s *response* (you usually don’t pre‑define this; you just read it). | N/A |\n",
      "\n",
      "**Tip:** Keep the system prompt *succinct* (≤ 1–2 sentences). The user prompt is where you give specifics.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Building an Effective Prompt\n",
      "\n",
      "#### 3.1 Start with a Clear Instruction\n",
      "\n",
      "```plaintext\n",
      "System: You are an expert math tutor who loves helping students solve problems step-by-step.\n",
      "User: The question is \"What is 45 × 23?\" Please work through the calculation and provide the final answer.\n",
      "```\n",
      "\n",
      "#### 3.2 Add Context or Data Explicitly\n",
      "\n",
      "If you need the model to read a paragraph, prepend it:\n",
      "\n",
      "```plaintext\n",
      "System: You are a summarizer that produces a 2‑sentence abstract in plain English.\n",
      "User: Text: \n",
      "  <your long paragraph here>\n",
      "Please output a concise summary.\n",
      "```\n",
      "\n",
      "#### 3.3 Use Examples (Few‑Shot Prompting)\n",
      "\n",
      "Give one or two examples before the real request. It trains the model to *copy the style*.\n",
      "\n",
      "```plaintext\n",
      "System: You are a code review assistant. Return only a diff in unified format.\n",
      "Example 1:\n",
      "User: Original: x = 5; print(x)\n",
      "      Modified: x = 5; print(x)\n",
      "      Diff: No changes.\n",
      "\n",
      "Now review:\n",
      "Original: y = 10; print(y)\n",
      "Modified: y = 12; print(y)\n",
      "```\n",
      "\n",
      "#### 3.4 Control the Output Format\n",
      "\n",
      "```plaintext\n",
      "System: Output strictly in JSON with keys \"status\" and \"message\".\n",
      "User: I need a reminder for my meeting tomorrow at 3 pm. Respond here.\n",
      "```\n",
      "\n",
      "#### 3.5 Encourage Step‑by‑Step Thinking (Chain‑of‑Thought)\n",
      "\n",
      "```plaintext\n",
      "System: Always think step-by-step before giving the final answer.\n",
      "User: What is the chemical formula of sodium chloride? Think through why.\n",
      "```\n",
      "\n",
      "---  \n",
      "\n",
      "### 4. Practical Prompt Templates\n",
      "\n",
      "| Use‑Case | Prompt Template |\n",
      "|----------|-----------------|\n",
      "| **Summarization** |  \n",
      "```plaintext\n",
      "System: You are a concise summarizer. Output up to 150 words, no extra formatting.\n",
      "User: The following article: <article>  Summarize it now.\n",
      "``` |\n",
      "| **Code Fix** |  \n",
      "```plaintext\n",
      "System: You are a senior Python developer. Return only the corrected code block.\n",
      "User: Original code: <code-snippet>   What needs to change?\n",
      "``` |\n",
      "| **Creative Story** |  \n",
      "```plaintext\n",
      "System: You are a creative writer. Use vivid imagery.\n",
      "User: Write a short story (≈ 300 words) about a cat who becomes a detective in Tokyo.\n",
      "``` |\n",
      "| **Math Problem** |  \n",
      "```plaintext\n",
      "System: You are math teacher. Solve the problem step-by-step. Do not skip steps.\n",
      "User: Solve 2x + 7 = 19 for x.\n",
      "``` |\n",
      "\n",
      "---  \n",
      "\n",
      "### 5. Managing Token Budget\n",
      "\n",
      "1. **Trim unnecessary fluff** – phrases like “Hey, please do X” can be shortened to “X”.\n",
      "2. **Summarize long context** – if you must give a lot of text, pre‑summarize it to < 1 k tokens.\n",
      "3. **Use placeholders** – pass only the IDs or short keys if the model can look up from memory (if you implement that).\n",
      "4. **Iterative approach** – break a huge request into several smaller ones; keep the output from earlier steps as input for the next.\n",
      "\n",
      "---  \n",
      "\n",
      "### 6. Common Pitfalls & Fixes\n",
      "\n",
      "| Pitfall | Symptoms | Fix |\n",
      "|---------|----------|-----|\n",
      "| “Hallucination” | Wrong facts, made‑up data | Prompt for verification: “Show the source if you know it, otherwise reply ‘I’m not sure’.” |\n",
      "| Lengthy replies | Exceeds token limit | Add a clause: “Summarize in exactly 3 sentences.” |\n",
      "| Missing details | Vague answers | Ask explicit follow‑up: “What specific value is requested?” |\n",
      "| Mixed formats | Confusing markdown, JSON, plain text | Define the format in the system prompt. |\n",
      "| Unnecessary repetition | Boilerplate “Sure, …” | Use “Reply only with the answer.” |\n",
      "\n",
      "---  \n",
      "\n",
      "### 7. Iterative Refinement Workflow\n",
      "\n",
      "1. **Draft** your prompt.  \n",
      "2. **Run** it once, capture the output.  \n",
      "3. **Evaluate** quality and format.  \n",
      "4. **Adjust** keywords, examples, or constraints.  \n",
      "5. **Repeat** until the response meets your target.\n",
      "\n",
      "A simple `while` loop in Python with the huggingface `transformers` `pipeline` can automate step 3 & 4.\n",
      "\n",
      "---  \n",
      "\n",
      "### 8. Safety & Ethics Note\n",
      "\n",
      "Because GPT‑OSS:20B does **not** enforce OpenAI‑style moderation or hallucination mitigation:\n",
      "\n",
      "- **Post‑process** the output to remove profanity or disallowed content.\n",
      "- **Vet** results that contain claims, especially legal or medical advice.\n",
      "- **Encourage transparency**: “If unsure, please say it.”\n",
      "\n",
      "---  \n",
      "\n",
      "## Quick Start\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
      "\n",
      "model_name = \"gpt-oss-20b\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
      "\n",
      "prompt = \"\"\"\n",
      "You are a helpful assistant who writes clean, well‑commented Python code. Only output the final code block, no explanation.\n",
      "\n",
      "Original code:\n",
      "def add(a,b):\n",
      "    return a - b\n",
      "What needs to be fixed?\n",
      "\"\"\"\n",
      "\n",
      "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
      "print(generator(prompt)[0][\"generated_text\"])\n",
      "```\n",
      "\n",
      "Replace the prompt text with your own use‑case, and you’re good to go.\n",
      "\n",
      "---  \n",
      "\n",
      "### Bottom‑Line\n",
      "\n",
      "- Start with a **clear system prompt**.  \n",
      "- Feed the **exact context** you want the model to consider.  \n",
      "- Use **few‑shot examples** if you need style or format.  \n",
      "- Keep everything **under the token limit** and **explicit about output**.  \n",
      "- Iterate, test, and refine until the output meets your standards.\n",
      "\n",
      "Happy prompting!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Local Ollama API\n",
    "    api_key=\"ollama\"                       # Dummy key\n",
    ")\n",
    " \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain how to prompt the gpt-oss:20b model effectively.\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
