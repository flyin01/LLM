{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360370ee",
   "metadata": {},
   "source": [
    "## Ollama local LLMs  \n",
    "\n",
    "This notebook is using Ollama and the following for running local LLMs:  \n",
    "* ollama sdk\n",
    "* openai sdk \n",
    "  \n",
    "Source 1: [Ollama Docs](https://docs.ollama.com/capabilities/tool-calling#python-2)  \n",
    "Source 2: [OpenaAI Cookbook](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)  \n",
    "  \n",
    "**Setup:**  \n",
    "1. To load a local model run the following from the terminal: `ollama run gpt-oss:20b`\n",
    "2. Run the notebook using venv and install libs from `requirements.txt`\n",
    "  \n",
    "### 1. Ollama SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9882de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\repo\\LLM\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b82d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '/requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r /requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9985ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45e8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa1bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Propose the optimal prompt for the gpt-oss:20b model.\n",
      "        Give an example of a poorly constructed prompt and improve it until it is optiomal.\n",
      "        The prompt should be usefull for agentic tasks with tool calling.\n",
      "        The response should be in markdown format.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model\n",
    "message = \"\"\"\n",
    "        Propose the optimal prompt for the gpt-oss:20b model.\n",
    "        Give an example of a poorly constructed prompt and improve it until it is optiomal.\n",
    "        The prompt should be usefull for agentic tasks with tool calling.\n",
    "        The response should be in markdown format.\n",
    "        \"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427369d5",
   "metadata": {},
   "source": [
    "#### Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519cf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: \n",
      " We need to propose an optimal prompt for gpt-oss:20b model. Provide an example of a poorly constructed prompt and improve it until optimal. The prompt should be useful for agentic tasks with tool calling. The response should be in markdown.\n",
      "\n",
      "We need to consider the gpt-oss:20b model: it's a 20-billion-parameter open-source model, presumably from the GPT-OSS family. We need to propose an optimal prompt: likely a structured prompt that encourages the model to act as an agent, making use of tool calling, and making sure it's concise, gives context, instructions, and clarifies tool usage. We can propose a prompt format: include system messages, user messages, clarifying that the model should think step by step, use tool calls. For GPT-OSS, tool calling support might be via \"function calls\" interface or \"tool calls\" similar to GPT-4.\n",
      "\n",
      "We should provide an example of a poorly constructed prompt, then show improved versions.\n",
      "\n",
      "We should respond in markdown format.\n",
      "\n",
      "We need to consider the difference between GPT-4 and GPT-OSS 20B: the 20B model is less powerful, but still large. Prompt should guide it. Might use \"You are an agent\" type of instructions. Also incorporate role and context. We should propose a structured approach: system: sets role, guidelines. Then maybe a user prompt: includes the task, a short context, and maybe tool list. The prompt should include a \"Tool calling\" instructions: e.g., \"When you need to fetch data, use the tool 'search' with arguments, etc.\"\n",
      "\n",
      "We might mention to use a \"tool\" format: \"CALL TOOL: tool_name(...)\" etc. In GPT-4 function calling, it's a structured JSON. For GPT-OSS, maybe tool calls are not natively supported, but we can emulate via instructions.\n",
      "\n",
      "We can propose a template like:\n",
      "\n",
      "```\n",
      "System: [role, guidelines, instruction on step-by-step reasoning, tool calling, etc.]\n",
      "\n",
      "User: [task description, context, list of available tools with signatures, instructions on how to call them]\n",
      "\n",
      "Assistant: [start of response: first step, maybe \"I will first think about the next step\", etc.]\n",
      "```\n",
      "\n",
      "But we want a single prompt that can be used by an agent to get better results. Possibly the prompt should include a small code snippet that sets up the environment for tool calling. However, as the prompt is to be used by the model, we cannot include code; we can instruct the model to produce certain formatted output.\n",
      "\n",
      "We might provide a \"tool call\" syntax: For example:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": \"search\",\n",
      "  \"arguments\": {\n",
      "    \"query\": \"example\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "The model will output that, and the orchestrator will interpret and call the tool.\n",
      "\n",
      "Alternatively, we can give a simple instruction: \"When you need to call a tool, output in JSON format: { tool: 'tool_name', arguments: { ... } }\".\n",
      "\n",
      "But for GPT-OSS:20b, tool calling might not be built-in, but the user can parse the output. So we can define the syntax.\n",
      "\n",
      "We also need to propose an example of a poorly constructed prompt and then refine it step by step.\n",
      "\n",
      "We'll produce a markdown answer:\n",
      "\n",
      "- Title: Optimal Prompt for gpt-oss:20b for Agentic Tool-Calling Tasks\n",
      "- Section: Overview\n",
      "- Section: Optimal Prompt Template\n",
      "- Section: Example: Poorly Constructed Prompt\n",
      "- Section: Step-by-Step Refinement (three iterations)\n",
      "- Section: Final Optimized Prompt\n",
      "- Additional notes.\n",
      "\n",
      "Make sure to emphasize the important parts: role specification, constraints, step-by-step reasoning, calling tool with proper syntax, context, clarity, limiting verbosity.\n",
      "\n",
      "We can also mention that we should keep the prompt size moderate to not exceed token budget; but for GPT-20B, prompt token limit may be ~8k or 32k tokens? We need to be aware. Usually GPT-3.5/4 have 8k or 32k. GPT-OSS:20b may have 8k. We can keep the prompt under 512 tokens.\n",
      "\n",
      "We can also mention the best practice of providing the tool list as part of the system message or user prompt: e.g., \"You have access to the following tools: search (searches the web), calculate (performs arithmetic), etc.\"\n",
      "\n",
      "Let's propose the optimal prompt:\n",
      "\n",
      "```\n",
      "**System Message**:\n",
      "You are a sophisticated language model trained to act as an autonomous agent. Your role is to assist the user by performing complex tasks using a set of tools. You must:\n",
      "1. Think carefully and plan your steps before taking action.\n",
      "2. Use the available tools only when necessary.\n",
      "3. Output tool calls in JSON format: { \"name\": \"tool_name\", \"arguments\": { ... } }.\n",
      "4. When a tool call completes, you will receive the tool's output as the next message. Use it to inform your next step.\n",
      "5. If no tool is needed, answer directly.\n",
      "6. Keep responses concise but complete.\n",
      "\n",
      "**Available Tools**:\n",
      "- `search`: arguments: `{\"query\": string}`; returns a string summary of top results.\n",
      "- `calculate`: arguments: `{\"expression\": string}`; returns the result as a string.\n",
      "- `translate`: arguments: `{\"text\": string, \"target_language\": string}`; returns the translation.\n",
      "- `summarize`: arguments: `{\"text\": string}`; returns a concise summary.\n",
      "\n",
      "**User Prompt**:\n",
      "[Insert user's task description and any context. Encourage the user to be specific about the desired outcome.]\n",
      "\n",
      "**Assistant**:\n",
      "(Here you begin your response, following the guidelines.)\n",
      "```\n",
      "\n",
      "But we need to propose the prompt that is passed to the model. Usually, you pass a message list with role: \"system\" and role: \"user\". So the prompt may be a combined system message and user message. But the question: \"Propose the optimal prompt for the gpt-oss:20b model.\" So we need to provide the actual prompt string, possibly as a \"system\" message that defines the guidelines, and then a \"user\" message that contains the task.\n",
      "\n",
      "However, we can produce a template that includes placeholders for the user input. Eg:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"role\": \"system\",\n",
      "  \"content\": \"... guidelines ...\",\n",
      "  \"tools\": [ ... ]\n",
      "}\n",
      "{\n",
      "  \"role\": \"user\",\n",
      "  \"content\": \"Task: ... Context: ...\"\n",
      "}\n",
      "```\n",
      "\n",
      "We should also show how to incorporate tool calling syntax.\n",
      "\n",
      "But we might also propose to use a single prompt with an instruction like: \"You are an agent. When you need to use a tool, output JSON like ...\". So we can propose a \"system\" message that instructs the model. The user message is then the task.\n",
      "\n",
      "Better to give a specific example: The prompt for the user might be: \"Find the top 5 companies with the highest market cap in 2023, and provide a summary.\" The model will think, decide to use search, produce tool call, then handle response.\n",
      "\n",
      "But the prompt we propose is generic, so the user can fill in the task. The prompt should instruct the model to be agentic and call tools accordingly.\n",
      "\n",
      "We should also propose a best practice: keep the prompt concise, but provide a clear structure for the model to parse. Use bullet lists for guidelines. Use examples.\n",
      "\n",
      "We also need to give an example of a poorly constructed prompt, then gradually improve it. For each iteration, show the prompt and explain why it's better.\n",
      "\n",
      "Potential poor prompts:\n",
      "\n",
      "- \"Help me find the best hotels in Paris.\" (too vague, no context, no instructions about tool usage)\n",
      "- \"Search for hotels.\" (lack of instruction on output formatting)\n",
      "- \"Write code.\" (lack of structure)\n",
      "- \"You are GPT, you can call search.\" (no guidelines, no step-by-step)\n",
      "\n",
      "We can refine to:\n",
      "\n",
      "- \"You are a helpful assistant. Use the search tool to find the best hotels in Paris and return them.\" (some improvement)\n",
      "- Then add step-by-step: \"Think first about next step. If you need to call a tool, output JSON.\"\n",
      "\n",
      "We can continue until it's optimal.\n",
      "\n",
      "We need to produce final answer in markdown. Provide the initial poor prompt, then improvements with numbered bullet points or steps.\n",
      "\n",
      "Let's structure the answer:\n",
      "\n",
      "1. Title: Optimal Prompt for GPT-OSS 20B with Tool Calling\n",
      "2. Section: Why a well-structured prompt matters\n",
      "3. Section: Optimal Prompt Template\n",
      "4. Section: Example of a poorly constructed prompt\n",
      "5. Section: Iterative refinement of the prompt\n",
      "   - Version 1 (Poor)\n",
      "   - Version 2 (First improvement)\n",
      "   - Version 3 (Second improvement)\n",
      "   - Version 4 (Third improvement)\n",
      "   - Version 5 (Final)\n",
      "6. Section: Final optimized prompt\n",
      "7. Section: Additional guidelines for use\n",
      "\n",
      "We need to show the prompts as code blocks in markdown.\n",
      "\n",
      "Also, we should include an explanation of each step of improvement: e.g., adding role, step-by-step reasoning, tool call format, context, etc.\n",
      "\n",
      "Also, maybe mention that the prompt includes the tool signature, so the model knows what arguments to provide.\n",
      "\n",
      "Additionally, maybe mention that the prompt should be short enough to fit within token limits. So we need to be careful about length.\n",
      "\n",
      "We can propose the prompt as two parts: system message (guidelines) and user message (task). The system message can include the tool list. The user message can be a placeholder.\n",
      "\n",
      "We also can provide a single combined prompt string that can be passed as a single message with role: \"system\" (if using new chat format). But we can propose the system message only.\n",
      "\n",
      "Let's think about the typical format for ChatGPT: messages: system, user, assistant. The system message sets the overall behavior. The user message is the user's request. So the prompt we propose will be the system message. Then the user provides the task. So we can give a system message that covers everything. Or we can propose that the system message includes tool definitions.\n",
      "\n",
      "The system message might be:\n",
      "\n",
      "```\n",
      "You are an AI assistant designed to act autonomously and use available tools to accomplish user tasks. Follow these rules:\n",
      "1. Think through the task step by step before acting.\n",
      "2. Only use a tool when necessary. Output a JSON object with the format { \"name\": \"<tool>\", \"arguments\": { ... } } for tool calls. No other output is allowed until the tool returns a response.\n",
      "3. Upon receiving a tool's response, incorporate it and continue. \n",
      "4. If no tool is needed, answer directly in plain text.\n",
      "5. Keep all responses concise but complete.\n",
      "Available tools:\n",
      "- search (args: { query: string }) returns a summary of top results.\n",
      "- calculate (args: { expression: string }) returns the result.\n",
      "- translate (args: { text: string, target_language: string }) returns translation.\n",
      "- summarize (args: { text: string }) returns concise summary.\n",
      "User's request: <placeholder>\n",
      "```\n",
      "\n",
      "We might mention that the user can just type the request, and the system message will fill in the rest. But the system message may be static except for the placeholder.\n",
      "\n",
      "Alternatively, we can propose a single prompt that includes the entire conversation. But it's better to separate system message from user message.\n",
      "\n",
      "Thus, the \"optimal prompt\" might be a system message plus a user message template. We can propose a single string that includes placeholders. For instance:\n",
      "\n",
      "```\n",
      "SYSTEM:\n",
      "You are an autonomous agent...\n",
      "TOOLS...\n",
      "When a tool is required, output JSON ...\n",
      "...\n",
      "\n",
      "USER:\n",
      "{user_query}\n",
      "```\n",
      "\n",
      "But for GPT-OSS:20b, maybe it's just a chat prompt with one message. But we can show as a multi-message array.\n",
      "\n",
      "Ok, let's produce a full answer. We'll write:\n",
      "\n",
      "```\n",
      "# Optimal Prompt for GPT-OSS:20b (Agentic Tasks with Tool Calling)\n",
      "\n",
      "## 1. Why a well‑structured prompt is critical\n",
      "...\n",
      "```\n",
      "\n",
      "Then provide the template.\n",
      "\n",
      "In the improvement section, show each iteration and why it's better.\n",
      "\n",
      "We must use markdown formatting.\n",
      "\n",
      "Now, think about how to illustrate the \"tool call\" output. We might show the model's response:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": \"search\",\n",
      "  \"arguments\": {\n",
      "    \"query\": \"best hotels in Paris 2024\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Then after the tool returns:\n",
      "\n",
      "```\n",
      "Top 5 hotels in Paris: ... \n",
      "```\n",
      "\n",
      "But the prompt itself needs to instruct the model on the format. So we must include the output format.\n",
      "\n",
      "Also mention that the model should not include extra text before or after the JSON; otherwise the orchestrator might misinterpret. So we instruct \"only output the JSON\" etc.\n",
      "\n",
      "Also mention that the model should handle errors and fallback to a direct answer if tool fails.\n",
      "\n",
      "Also mention that the model should consider privacy, e.g., avoid disclosing personal data, etc. But that might be optional.\n",
      "\n",
      "We can also mention that the prompt includes the \"role\" and \"function\" of each tool, so the model can plan.\n",
      "\n",
      "We should also note that the prompt may contain \"You are the assistant\" to help model.\n",
      "\n",
      "Ok, let's outline the answer:\n",
      "\n",
      "1. Title\n",
      "2. Explanation\n",
      "3. Optimal Prompt Template\n",
      "4. Example of Poor Prompt\n",
      "5. Iterative refinement\n",
      "6. Final Prompt\n",
      "7. Additional Tips\n",
      "\n",
      "Within the refinement, we can show:\n",
      "\n",
      "- Poor: \"Find best hotels in Paris\"\n",
      "- First improvement: \"Help me find best hotels in Paris.\"\n",
      "- Second improvement: \"I need the top 5 hotels in Paris for July 2024.\"\n",
      "- Third improvement: \"Use the search tool. Output JSON. Etc.\"\n",
      "\n",
      "We should also highlight the steps: adding tool list, JSON output, step-by-step, context, instructions for the model.\n",
      "\n",
      "Ok, let's write.\n",
      "\n",
      "We'll produce a markdown answer with code blocks for each prompt.\n",
      "\n",
      "We'll also include the final prompt in a code block.\n",
      "\n",
      "Also note that GPT-OSS:20b might not have \"function calling\" but we can instruct the model to produce the JSON, and the orchestrator can parse.\n",
      "\n",
      "Also mention that the prompt should keep tokens low, so we don't include huge tool descriptions.\n",
      "\n",
      "Ok, let's write the answer. \n",
      "\n",
      "Answer: \n",
      " # Optimal Prompt for **gpt‑oss:20b** – Agentic Tasks with Tool Calling  \n",
      "> **Goal:** Create a single, reusable prompt that turns GPT‑OSS 20B into a reliable autonomous agent that can think step‑by‑step, decide when to call tools, and format tool calls in a machine‑readable way.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Why a Well‑Structured Prompt Matters  \n",
      "\n",
      "| Challenge | Impact | How a Good Prompt Helps |\n",
      "|-----------|--------|------------------------|\n",
      "| **Ambiguous instructions** | The model may guess, producing irrelevant or noisy output. | Explicit rules (`think first`, `output JSON`, `only JSON when calling a tool`) remove guesswork. |\n",
      "| **Tool misuse or missing arguments** | Tool calls fail or return incomplete data. | Providing a *tool signature* in the prompt lets the model know exactly what arguments to supply. |\n",
      "| **Excess verbosity** | Tokens are wasted, hitting limits and lowering response quality. | Concise, bullet‑pointed guidelines keep the prompt short. |\n",
      "| **Inconsistent format** | The orchestrator cannot parse the response. | A strict “only JSON” rule guarantees parseability. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. The Optimal Prompt Template  \n",
      "\n",
      "> **Structure**: Two messages – a *system* message that sets the behavior and tool list, and a *user* message that contains the actual request.  \n",
      "> **Length**: < 300 tokens (≈ 150 words).  \n",
      "> **Tool Call Format** (JSON, no surrounding text):  \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"<tool_name>\",\n",
      "  \"arguments\": { ... }\n",
      "}\n",
      "```\n",
      "\n",
      "### System Message (≈ 150 words)\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "### User Message (Insert the actual task)\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "> *When you send the prompt to the model, replace the user message with the real request.*\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Example of a Poorly Constructed Prompt  \n",
      "\n",
      "| Prompt | Why It’s Poor |\n",
      "|--------|---------------|\n",
      "| `\"Help me find the best hotels in Paris.\"` | 1. Too vague – no date, no tool specification. 2. No instruction on JSON format. 3. The model might output plain text, breaking the orchestrator. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Iterative Refinement of the Prompt  \n",
      "\n",
      "| Iteration | Prompt | What Changed | Reason |\n",
      "|-----------|--------|--------------|--------|\n",
      "| **V1 – Basic** | `\"Find the best hotels in Paris.\"` | Added minimal detail. | Still ambiguous; no tool usage guidance. |\n",
      "| **V2 – Adding Context** | `\"Find the best hotels in Paris for July 2024.\"` | Date added. | Better specificity, but still no tool instructions. |\n",
      "| **V3 – Specifying Tool Call** | `\"Use the search tool to find the best hotels in Paris for July 2024. Return the top 5 hotels with a short description.\"` | Explicitly instructs tool usage, but no JSON format. | Good start, but tool output might still be wrapped in prose. |\n",
      "| **V4 – Enforcing JSON Format** | `\"Please call the `search` tool with query: 'top hotels in Paris July 2024'. Return only the JSON object: `{ \"name\": \"search\", \"arguments\": { \"query\": \"top hotels in Paris July 2024\" } }`. No other text.\"` | Explicit JSON format and tool name. | Ensures parseability but now too mechanical; missing planning step. |\n",
      "| **V5 – Full System Prompt** | *The system message described in Section 2.* | Adds planning, error handling, concise answers, and a clean tool signature. | Balances flexibility (the model can decide to plan) with strict output formatting. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Final Optimized Prompt  \n",
      "\n",
      "> **Copy‑paste this as the *system* message** (the first element of the chat).  \n",
      "> **Then provide the user’s request as the second message**.\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "User message (example):\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick Reference Cheat‑Sheet  \n",
      "\n",
      "| Step | What the Model Does | What It Outputs |\n",
      "|------|---------------------|-----------------|\n",
      "| **Planning** | \"I need to decide what tool to use.\" | None (internal thought) |\n",
      "| **Tool Call** | Calls `search`. | JSON object only, e.g. `{\"name\":\"search\",\"arguments\":{\"query\":\"top hotels in Paris July 2024\"}}` |\n",
      "| **Tool Result Received** | Processes text, decides next action. | If no tool needed: plain text answer. If more tools needed: another JSON. |\n",
      "| **Error** | Detects tool error. | Plain text explanation, optionally new plan. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Additional Tips  \n",
      "\n",
      "1. **Keep the system message short** – If you hit the token limit, trim verbose explanations.  \n",
      "2. **Avoid unnecessary words** – GPT‑OSS 20B is sensitive to prompt length.  \n",
      "3. **Test with a sandbox** – Simulate tool responses to see how the model reacts before deploying.  \n",
      "4. **Update tool list as needed** – Add new tools by inserting rows in the table; the model will automatically know the signature.  \n",
      "5. **Use a prompt manager** – Store the system message in a constant so you never forget the formatting rules.  \n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought  \n",
      "\n",
      "With this prompt, GPT‑OSS 20B behaves like a disciplined, tool‑using agent: it plans, calls the right tool with the right arguments, and hands back clean, parseable JSON that your orchestrator can act on. This reduces error rates, saves tokens, and gives you a robust foundation for building sophisticated agentic workflows.\n"
     ]
    }
   ],
   "source": [
    "# Define the chat interaction with the model. For tool calling, need think=True\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=[{'role': 'user',\n",
    "               'content':message}],\n",
    "               think = \"high\", #True/False. Except for gpt-oss use levels: \"low\", \"medium\", \"high\"\n",
    "               stream=False\n",
    ")\n",
    "\n",
    "print('Thinking: \\n', response.message.thinking, '\\n')\n",
    "print('Answer: \\n', response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6483fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Optimal Prompt for **gpt‑oss:20b** – Agentic Tasks with Tool Calling  \n",
      "> **Goal:** Create a single, reusable prompt that turns GPT‑OSS 20B into a reliable autonomous agent that can think step‑by‑step, decide when to call tools, and format tool calls in a machine‑readable way.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Why a Well‑Structured Prompt Matters  \n",
      "\n",
      "| Challenge | Impact | How a Good Prompt Helps |\n",
      "|-----------|--------|------------------------|\n",
      "| **Ambiguous instructions** | The model may guess, producing irrelevant or noisy output. | Explicit rules (`think first`, `output JSON`, `only JSON when calling a tool`) remove guesswork. |\n",
      "| **Tool misuse or missing arguments** | Tool calls fail or return incomplete data. | Providing a *tool signature* in the prompt lets the model know exactly what arguments to supply. |\n",
      "| **Excess verbosity** | Tokens are wasted, hitting limits and lowering response quality. | Concise, bullet‑pointed guidelines keep the prompt short. |\n",
      "| **Inconsistent format** | The orchestrator cannot parse the response. | A strict “only JSON” rule guarantees parseability. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. The Optimal Prompt Template  \n",
      "\n",
      "> **Structure**: Two messages – a *system* message that sets the behavior and tool list, and a *user* message that contains the actual request.  \n",
      "> **Length**: < 300 tokens (≈ 150 words).  \n",
      "> **Tool Call Format** (JSON, no surrounding text):  \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"<tool_name>\",\n",
      "  \"arguments\": { ... }\n",
      "}\n",
      "```\n",
      "\n",
      "### System Message (≈ 150 words)\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "### User Message (Insert the actual task)\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "> *When you send the prompt to the model, replace the user message with the real request.*\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Example of a Poorly Constructed Prompt  \n",
      "\n",
      "| Prompt | Why It’s Poor |\n",
      "|--------|---------------|\n",
      "| `\"Help me find the best hotels in Paris.\"` | 1. Too vague – no date, no tool specification. 2. No instruction on JSON format. 3. The model might output plain text, breaking the orchestrator. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Iterative Refinement of the Prompt  \n",
      "\n",
      "| Iteration | Prompt | What Changed | Reason |\n",
      "|-----------|--------|--------------|--------|\n",
      "| **V1 – Basic** | `\"Find the best hotels in Paris.\"` | Added minimal detail. | Still ambiguous; no tool usage guidance. |\n",
      "| **V2 – Adding Context** | `\"Find the best hotels in Paris for July 2024.\"` | Date added. | Better specificity, but still no tool instructions. |\n",
      "| **V3 – Specifying Tool Call** | `\"Use the search tool to find the best hotels in Paris for July 2024. Return the top 5 hotels with a short description.\"` | Explicitly instructs tool usage, but no JSON format. | Good start, but tool output might still be wrapped in prose. |\n",
      "| **V4 – Enforcing JSON Format** | `\"Please call the `search` tool with query: 'top hotels in Paris July 2024'. Return only the JSON object: `{ \"name\": \"search\", \"arguments\": { \"query\": \"top hotels in Paris July 2024\" } }`. No other text.\"` | Explicit JSON format and tool name. | Ensures parseability but now too mechanical; missing planning step. |\n",
      "| **V5 – Full System Prompt** | *The system message described in Section 2.* | Adds planning, error handling, concise answers, and a clean tool signature. | Balances flexibility (the model can decide to plan) with strict output formatting. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Final Optimized Prompt  \n",
      "\n",
      "> **Copy‑paste this as the *system* message** (the first element of the chat).  \n",
      "> **Then provide the user’s request as the second message**.\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "User message (example):\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick Reference Cheat‑Sheet  \n",
      "\n",
      "| Step | What the Model Does | What It Outputs |\n",
      "|------|---------------------|-----------------|\n",
      "| **Planning** | \"I need to decide what tool to use.\" | None (internal thought) |\n",
      "| **Tool Call** | Calls `search`. | JSON object only, e.g. `{\"name\":\"search\",\"arguments\":{\"query\":\"top hotels in Paris July 2024\"}}` |\n",
      "| **Tool Result Received** | Processes text, decides next action. | If no tool needed: plain text answer. If more tools needed: another JSON. |\n",
      "| **Error** | Detects tool error. | Plain text explanation, optionally new plan. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Additional Tips  \n",
      "\n",
      "1. **Keep the system message short** – If you hit the token limit, trim verbose explanations.  \n",
      "2. **Avoid unnecessary words** – GPT‑OSS 20B is sensitive to prompt length.  \n",
      "3. **Test with a sandbox** – Simulate tool responses to see how the model reacts before deploying.  \n",
      "4. **Update tool list as needed** – Add new tools by inserting rows in the table; the model will automatically know the signature.  \n",
      "5. **Use a prompt manager** – Store the system message in a constant so you never forget the formatting rules.  \n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought  \n",
      "\n",
      "With this prompt, GPT‑OSS 20B behaves like a disciplined, tool‑using agent: it plans, calls the right tool with the right arguments, and hands back clean, parseable JSON that your orchestrator can act on. This reduces error rates, saves tokens, and gives you a robust foundation for building sophisticated agentic workflows.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dec4a",
   "metadata": {},
   "source": [
    "#### Define Tools for LLM\n",
    "What improves tool usage accuracy.  \n",
    "Ranked by importance:\n",
    "1. Clear parameter names\n",
    "2. Clear docstring\n",
    "3. Simple signature\n",
    "4. Stable return format (JSON-serializable)\n",
    "5. Clean error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5532f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tool 1 - Improved according to above\n",
    "def square_number(number: int) -> int:\n",
    "    \"\"\"Compute the square of a single integer.\n",
    "    Args:\n",
    "        number (int): An integer value to be squared.\n",
    "\n",
    "    Returns:\n",
    "        result (int): The square of the input number.\n",
    "    \"\"\"\n",
    "    if not isinstance(number, int):\n",
    "        raise ValueError(\"number must be an integer.\")\n",
    "    result = number * number\n",
    "    \n",
    "    return result\n",
    "\n",
    "square_number(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a79ecfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BILLY'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tool 2 - IMPROVE THIS FUNCTION - TBD\n",
    "def select_item(number: int) -> str:\n",
    "    \"\"\"This function returns an item (str) from a list of items based on the number provided.\n",
    "    Args:\n",
    "        number (int): The the number is the index of the item to be selected from a list of items.\n",
    "\n",
    "    Returns:\n",
    "        item (str): The selected item from the list.    \n",
    "    \"\"\"\n",
    "    if not isinstance(number, int):\n",
    "        raise ValueError(\"number must be an integer.\")\n",
    "\n",
    "    items: list[str] = ['BILLY', 'PAX', 'MARKUS', 'MICKE', 'ALGOT']\n",
    "\n",
    "    if number < 1 or number > len(items):\n",
    "        raise IndexError(\"number out of range.\")\n",
    "\n",
    "    return items[number - 1]\n",
    "\n",
    "select_item(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a436e",
   "metadata": {},
   "source": [
    "#### Tools calling alt 1 - One tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0170d6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking:\n",
      " We have a user request: \"Compute the square of 3 using the available tool. Only return the numeric result.\" There's a tool named \"square_number\" that computes the square of a single integer. So we must call the function \"square_number\" with argument number: 3. Then we only return the numeric result. The user wants only numeric result, so we should output just the number 9.\n",
      "\n",
      "We must use the function tool: we need to produce a function call: {\"name\":\"square_number\",\"arguments\":{...}}.\n",
      "\n",
      "We should not include any other text. Just the function call? The instructions say \"Only return the numeric result.\" But the tool system requires us to call the function first. So we must call the function. Then the system will respond with the result. However, the user only wants numeric result. But we can't skip the function call. According to guidelines: We must call the tool. The tool returns something like 9. The system will produce output? We need to decide: In the conversation we call the function. After calling, we get the result from the tool. Then we provide the numeric result. But the instruction \"Only return the numeric result\" likely refers to the final answer after calling the tool. But the assistant should still show the function call. Actually the system expects a function call. Then the function will return the numeric result, and the assistant will respond with that numeric result. So we can just call the function and output the result. The final response should be just \"9\".\n",
      "\n",
      "Thus the sequence: Assistant calls function with number=3. Then system returns 9. Assistant should respond \"9\". So we need to produce two messages? The first message calls the function. The second message returns the numeric result. But the instructions: \"Only return the numeric result.\" It could be interpreted that we must output just the numeric result after tool call. So we might just do the function call and then the final output numeric result. In the final output, we must not include extraneous text. So after tool call, the assistant will produce a message with just \"9\".\n",
      "\n",
      "Thus we need to produce a function call: name: square_number, arguments: { number: 3 }. The tool will respond with 9. Then the assistant's final message is 9.\n",
      "\n",
      "So let's do that.\n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model and use the tool\n",
    "message = \"\"\"\n",
    "        Compute the square of 3 using the available tool. Only return the numeric result.\n",
    "\"\"\"\n",
    "messages = [{'role': 'user', 'content': message}]\n",
    "\n",
    "# Pass a Python functions diretcly as tools (or alternatively provide a JSON schema)\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=messages,\n",
    "    tools=[square_number],\n",
    "    think=\"high\") #True/False. Except for gpt-oss use levels: \"low\", \"medium\", \"high\"\n",
    "\n",
    "# Adds the model's response to the messages list. To keep the chat context up to date\n",
    "messages.append(response.message)\n",
    "\n",
    "# Debug only. Not to be used in production.\n",
    "if hasattr(response.message, \"thinking\"):\n",
    "    print(\"Thinking:\\n\", response.message.thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08b7bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if response.message.tool_calls:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0c76e",
   "metadata": {},
   "source": [
    "##### Break down step-by-step of Tools calling alt 1 - One tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94de306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "{'role': 'user', 'content': '\\n        Compute the square of 3 using the available tool. Only return the numeric result.\\n'}\n",
      "user\n",
      "\n",
      "        Compute the square of 3 using the available tool. Only return the numeric result.\n",
      "\n",
      "role='assistant' content='' thinking='We have a user request: \"Compute the square of 3 using the available tool. Only return the numeric result.\" There\\'s a tool named \"square_number\" that computes the square of a single integer. So we must call the function \"square_number\" with argument number: 3. Then we only return the numeric result. The user wants only numeric result, so we should output just the number 9.\\n\\nWe must use the function tool: we need to produce a function call: {\"name\":\"square_number\",\"arguments\":{...}}.\\n\\nWe should not include any other text. Just the function call? The instructions say \"Only return the numeric result.\" But the tool system requires us to call the function first. So we must call the function. Then the system will respond with the result. However, the user only wants numeric result. But we can\\'t skip the function call. According to guidelines: We must call the tool. The tool returns something like 9. The system will produce output? We need to decide: In the conversation we call the function. After calling, we get the result from the tool. Then we provide the numeric result. But the instruction \"Only return the numeric result\" likely refers to the final answer after calling the tool. But the assistant should still show the function call. Actually the system expects a function call. Then the function will return the numeric result, and the assistant will respond with that numeric result. So we can just call the function and output the result. The final response should be just \"9\".\\n\\nThus the sequence: Assistant calls function with number=3. Then system returns 9. Assistant should respond \"9\". So we need to produce two messages? The first message calls the function. The second message returns the numeric result. But the instructions: \"Only return the numeric result.\" It could be interpreted that we must output just the numeric result after tool call. So we might just do the function call and then the final output numeric result. In the final output, we must not include extraneous text. So after tool call, the assistant will produce a message with just \"9\".\\n\\nThus we need to produce a function call: name: square_number, arguments: { number: 3 }. The tool will respond with 9. Then the assistant\\'s final message is 9.\\n\\nSo let\\'s do that.' images=None tool_name=None tool_calls=[ToolCall(function=Function(name='square_number', arguments={'number': 3}))]\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Break down \n",
    "print(type(messages))\n",
    "print(len(messages))\n",
    "print(messages[0])\n",
    "print(messages[0]['role'])\n",
    "print(messages[0]['content'])\n",
    "print(messages[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac5e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function=Function(name='square_number', arguments={'number': 3})\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Break down \n",
    "print(response.message.tool_calls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52801e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function=Function(name='square_number', arguments={'number': 3})\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Break down \n",
    "call = response.message.tool_calls[0]\n",
    "print(call)\n",
    "result = square_number(**call.function.arguments)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb256dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Break down \n",
    "\n",
    "# add the tool result to the messages\n",
    "messages.append({\"role\": \"tool\", \n",
    "                 \"tool_name\": call.function.name, \n",
    "                 \"content\": str(result)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c0461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n",
      "{'role': 'user', 'content': '\\n        Compute the square of 3 using the available tool. Only return the numeric result.\\n'}\n",
      "user\n",
      "\n",
      "        Compute the square of 3 using the available tool. Only return the numeric result.\n",
      "\n",
      "role='assistant' content='' thinking='We have a user request: \"Compute the square of 3 using the available tool. Only return the numeric result.\" There\\'s a tool named \"square_number\" that computes the square of a single integer. So we must call the function \"square_number\" with argument number: 3. Then we only return the numeric result. The user wants only numeric result, so we should output just the number 9.\\n\\nWe must use the function tool: we need to produce a function call: {\"name\":\"square_number\",\"arguments\":{...}}.\\n\\nWe should not include any other text. Just the function call? The instructions say \"Only return the numeric result.\" But the tool system requires us to call the function first. So we must call the function. Then the system will respond with the result. However, the user only wants numeric result. But we can\\'t skip the function call. According to guidelines: We must call the tool. The tool returns something like 9. The system will produce output? We need to decide: In the conversation we call the function. After calling, we get the result from the tool. Then we provide the numeric result. But the instruction \"Only return the numeric result\" likely refers to the final answer after calling the tool. But the assistant should still show the function call. Actually the system expects a function call. Then the function will return the numeric result, and the assistant will respond with that numeric result. So we can just call the function and output the result. The final response should be just \"9\".\\n\\nThus the sequence: Assistant calls function with number=3. Then system returns 9. Assistant should respond \"9\". So we need to produce two messages? The first message calls the function. The second message returns the numeric result. But the instructions: \"Only return the numeric result.\" It could be interpreted that we must output just the numeric result after tool call. So we might just do the function call and then the final output numeric result. In the final output, we must not include extraneous text. So after tool call, the assistant will produce a message with just \"9\".\\n\\nThus we need to produce a function call: name: square_number, arguments: { number: 3 }. The tool will respond with 9. Then the assistant\\'s final message is 9.\\n\\nSo let\\'s do that.' images=None tool_name=None tool_calls=[ToolCall(function=Function(name='square_number', arguments={'number': 3}))]\n",
      "{'role': 'tool', 'tool_name': 'square_number', 'content': '9'}\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - Break down \n",
    "\n",
    "# debug prints after adding tool result\n",
    "print(type(messages))\n",
    "print(len(messages))\n",
    "print(messages[0])\n",
    "print(messages[0]['role'])\n",
    "print(messages[0]['content'])\n",
    "print(messages[1])\n",
    "print(messages[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0389e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Thinking: \n",
      " None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only recommended for models which only return a single tool call\n",
    "messages.append(response.message)\n",
    "if response.message.tool_calls:\n",
    "\n",
    "  call = response.message.tool_calls[0] # get function call - function=Function(name='square_number', arguments={'number': 2})\n",
    "  result = square_number(**call.function.arguments) # run the tool with the arguments\n",
    "\n",
    "  # add the tool result to the messages - appends another item to the list\n",
    "  messages.append({\"role\": \"tool\", \n",
    "                   \"tool_name\": call.function.name, \n",
    "                   \"content\": str(result)})\n",
    "\n",
    "  final_response = chat(model=\"gpt-oss:20b\", \n",
    "                        messages=messages, \n",
    "                        tools=[square_number], \n",
    "                        think=\"high\")\n",
    "\n",
    "  print('Final Thinking: \\n', final_response.message.thinking, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cb26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='9' thinking=None images=None tool_name=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Break down \n",
    "\n",
    "print(final_response.message) # Why is think none?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f9115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# Step 7 - Break down - END OF BREAKDOWN \n",
    "\n",
    "print(final_response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3740c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking:\n",
      " The user says: \"Compute the square of 3 using the available tool. Return only the numeric result.\"\n",
      "\n",
      "We must call the square_number function with number: 3. Then return only numeric result. So we need to call the function via the \"functions\" tool. We'll do that. \n",
      "\n",
      "Final Answer: 9\n"
     ]
    }
   ],
   "source": [
    "# Improved version (tool complete)\n",
    "# It seems that the model needs to be called twice - once for tool planning \n",
    "# and once for final response after tool execution\n",
    "\n",
    "# User prompt\n",
    "message = \"Compute the square of 3 using the available tool. Return only the numeric result.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "# First model call (tool planning)\n",
    "response = chat(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=messages,\n",
    "    tools=[square_number],\n",
    "    think=\"high\"\n",
    ")\n",
    "\n",
    "messages.append(response.message)\n",
    "\n",
    "# Debug: internal reasoning (optional)\n",
    "if hasattr(response.message, \"thinking\"):\n",
    "    print(\"Thinking:\\n\", response.message.thinking, \"\\n\") # Print thinking process before tool calling\n",
    "\n",
    "# Tool execution\n",
    "if response.message.tool_calls:\n",
    "    call = response.message.tool_calls[0] # get function call - function=Function(name='square_number', arguments={'number': 3})\n",
    "\n",
    "    result = square_number(**call.function.arguments) # run the tool with the arguments\n",
    "\n",
    "    # Inject tool result\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_name\": call.function.name,\n",
    "        \"content\": str(result)\n",
    "    })\n",
    "\n",
    "    # Final model response\n",
    "    final_response = chat(\n",
    "        model=\"gpt-oss:20b\",\n",
    "        messages=messages,\n",
    "        tools=[square_number],\n",
    "        think=\"high\"\n",
    "    )\n",
    "\n",
    "    print(\"Final Answer:\", final_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81a39f",
   "metadata": {},
   "source": [
    "##### Tools calling alt 1 - DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e898395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='4' thinking='User asks: \"What is the square of 2?\" Only return numeric result. So we should compute 2 squared = 4. But we also have tool square_number. They want numeric result. We can just call the function? The instructions say \"Only return the numeric result.\" So we should produce just \"4\". We can use the tool or not. According to guidelines, when using tool we produce a tool call JSON. But user expects only numeric result. We might not need to use tool. It\\'s simpler: just output 4. But guidelines: The user didn\\'t ask for the function output. But we can still compute ourselves. The result is 4. We\\'ll output \"4\".' images=None tool_name=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "# print('Answer: \\n', response.message.content) # There is no content now?\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug prints\n",
    "print(type(messages))\n",
    "print(len(messages))\n",
    "print(messages[0])\n",
    "print(messages[0]['role'])\n",
    "print(messages[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4581def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ollama._types.ChatResponse'>\n",
      "Help on ChatResponse in module ollama._types object:\n",
      "\n",
      "class ChatResponse(BaseGenerateResponse)\n",
      " |  ChatResponse(\n",
      " |      *,\n",
      " |      model: Optional[str] = None,\n",
      " |      created_at: Optional[str] = None,\n",
      " |      done: Optional[bool] = None,\n",
      " |      done_reason: Optional[str] = None,\n",
      " |      total_duration: Optional[int] = None,\n",
      " |      load_duration: Optional[int] = None,\n",
      " |      prompt_eval_count: Optional[int] = None,\n",
      " |      prompt_eval_duration: Optional[int] = None,\n",
      " |      eval_count: Optional[int] = None,\n",
      " |      eval_duration: Optional[int] = None,\n",
      " |      message: ollama._types.Message\n",
      " |  ) -> None\n",
      " |\n",
      " |  Response returned by chat requests.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ChatResponse\n",
      " |      BaseGenerateResponse\n",
      " |      SubscriptableBaseModel\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'message': <class 'ollama._types.Message'>}\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      " |\n",
      " |  __pydantic_custom_init__ = False\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'created_at': FieldInfo(annotation=Union[str, N...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ChatResponse\", validat...\n",
      " |\n",
      " |  __signature__ = <Signature (*, model: Optional[str] = None, crea... = ...\n",
      " |\n",
      " |  model_config = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __contains__(self, key: str) -> bool\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> 'nonexistent' in msg\n",
      " |      False\n",
      " |      >>> 'role' in msg\n",
      " |      True\n",
      " |      >>> 'content' in msg\n",
      " |      False\n",
      " |      >>> msg.content = 'hello!'\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> msg = Message(role='user', content='hello!')\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      False\n",
      " |      >>> msg['tool_calls'] = []\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = None\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> tool = Tool()\n",
      " |      >>> 'type' in tool\n",
      " |      True\n",
      " |\n",
      " |  __getitem__(self, key: str) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role']\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['nonexistent']\n",
      " |      Traceback (most recent call last):\n",
      " |      KeyError: 'nonexistent'\n",
      " |\n",
      " |  __setitem__(self, key: str, value: Any) -> None\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role'] = 'assistant'\n",
      " |      >>> msg['role']\n",
      " |      'assistant'\n",
      " |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      " |      >>> msg = Message(role='user', content='hello')\n",
      " |      >>> msg['tool_calls'] = [tool_call]\n",
      " |      >>> msg['tool_calls'][0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  get(self, key: str, default: Any = None) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('role')\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent')\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent', 'default')\n",
      " |      'default'\n",
      " |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      " |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |\n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      update: 'Dict[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False\n",
      " |  ) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      encoder: 'Callable[[Any], Any] | None' = PydanticUndefined,\n",
      " |      models_as_dict: 'bool' = PydanticUndefined,\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  model_copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      update: 'Mapping[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/models.md#model-copy)\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(\n",
      " |      self,\n",
      " |      *,\n",
      " |      mode: \"Literal['json', 'python'] | str\" = 'python',\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      indent: 'int | None' = None,\n",
      " |      ensure_ascii: 'bool' = False,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
      " |              If `False` (the default), these characters will be output as-is.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |\n",
      " |  __get_pydantic_core_schema__(\n",
      " |      source: 'type[BaseModel]',\n",
      " |      handler: 'GetCoreSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'CoreSchema'\n",
      " |\n",
      " |  __get_pydantic_json_schema__(\n",
      " |      core_schema: 'CoreSchema',\n",
      " |      handler: 'GetJsonSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
      " |      meaning that creating an instance of the class may fail.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by Pydantic.\n",
      " |\n",
      " |      Note:\n",
      " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
      " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
      " |\n",
      " |  __pydantic_on_complete__() -> 'None'\n",
      " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
      " |\n",
      " |      This typically happens when the class is created (just before\n",
      " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
      " |      except when forward annotations are used that could not immediately be resolved.\n",
      " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
      " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>,\n",
      " |      mode: 'JsonSchemaMode' = 'validation',\n",
      " |      *,\n",
      " |      union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of'\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
      " |\n",
      " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
      " |              keyword to combine schemas (the default).\n",
      " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
      " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
      " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
      " |              `any_of`.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(\n",
      " |      *,\n",
      " |      force: 'bool' = False,\n",
      " |      raise_errors: 'bool' = True,\n",
      " |      _parent_namespace_depth: 'int' = 2,\n",
      " |      _types_namespace: 'MappingNamespace | None' = None\n",
      " |  ) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      from_attributes: 'bool | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(\n",
      " |      json_data: 'str | bytes | bytearray',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(\n",
      " |      path: 'str | Path',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(\n",
      " |      b: 'str | bytes',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(\n",
      " |      *,\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_fields = {'created_at': FieldInfo(annotation=Union[str, NoneType...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# debug prints\n",
    "print(type(response))\n",
    "help(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-oss:20b\n",
      "2025-12-26T21:05:27.5932441Z\n",
      "True\n",
      "stop\n",
      "624075300\n",
      "148588300\n",
      "149\n",
      "103599800\n",
      "42\n",
      "356871600\n",
      "role='assistant' content='' thinking='We need to call the tool square_nums with number=2. Then return the result.' images=None tool_name=None tool_calls=[ToolCall(function=Function(name='square_nums', arguments={'number': 2}))]\n"
     ]
    }
   ],
   "source": [
    "# inspect response prints\n",
    "print(response.model)\n",
    "print(response.created_at)\n",
    "print(response.done)\n",
    "print(response.done_reason)\n",
    "print(response.total_duration)\n",
    "print(response.load_duration)\n",
    "print(response.prompt_eval_count)\n",
    "print(response.prompt_eval_duration)\n",
    "print(response.eval_count)\n",
    "print(response.eval_duration)\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cbd0950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ollama._types.Message'>\n",
      "Help on Message in module ollama._types object:\n",
      "\n",
      "class Message(SubscriptableBaseModel)\n",
      " |  Message(\n",
      " |      *,\n",
      " |      role: str,\n",
      " |      content: Optional[str] = None,\n",
      " |      thinking: Optional[str] = None,\n",
      " |      images: Optional[Sequence[ollama._types.Image]] = None,\n",
      " |      tool_name: Optional[str] = None,\n",
      " |      tool_calls: Optional[Sequence[ollama._types.Message.ToolCall]] = None\n",
      " |  ) -> None\n",
      " |\n",
      " |  Chat message.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Message\n",
      " |      SubscriptableBaseModel\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  ToolCall = <class 'ollama._types.Message.ToolCall'>\n",
      " |      Model tool calls.\n",
      " |\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'content': typing.Optional[str], 'images': typing.O...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      " |\n",
      " |  __pydantic_custom_init__ = False\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'content': FieldInfo(annotation=Union[str, None...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"Message\", validator=Mo...\n",
      " |\n",
      " |  __signature__ = <Signature (*, role: str, content: Optional[str]...oll...\n",
      " |\n",
      " |  model_config = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __contains__(self, key: str) -> bool\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> 'nonexistent' in msg\n",
      " |      False\n",
      " |      >>> 'role' in msg\n",
      " |      True\n",
      " |      >>> 'content' in msg\n",
      " |      False\n",
      " |      >>> msg.content = 'hello!'\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> msg = Message(role='user', content='hello!')\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      False\n",
      " |      >>> msg['tool_calls'] = []\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = None\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> tool = Tool()\n",
      " |      >>> 'type' in tool\n",
      " |      True\n",
      " |\n",
      " |  __getitem__(self, key: str) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role']\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['nonexistent']\n",
      " |      Traceback (most recent call last):\n",
      " |      KeyError: 'nonexistent'\n",
      " |\n",
      " |  __setitem__(self, key: str, value: Any) -> None\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role'] = 'assistant'\n",
      " |      >>> msg['role']\n",
      " |      'assistant'\n",
      " |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      " |      >>> msg = Message(role='user', content='hello')\n",
      " |      >>> msg['tool_calls'] = [tool_call]\n",
      " |      >>> msg['tool_calls'][0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  get(self, key: str, default: Any = None) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('role')\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent')\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent', 'default')\n",
      " |      'default'\n",
      " |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      " |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |\n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      update: 'Dict[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False\n",
      " |  ) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      encoder: 'Callable[[Any], Any] | None' = PydanticUndefined,\n",
      " |      models_as_dict: 'bool' = PydanticUndefined,\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  model_copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      update: 'Mapping[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/models.md#model-copy)\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(\n",
      " |      self,\n",
      " |      *,\n",
      " |      mode: \"Literal['json', 'python'] | str\" = 'python',\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      indent: 'int | None' = None,\n",
      " |      ensure_ascii: 'bool' = False,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
      " |              If `False` (the default), these characters will be output as-is.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |\n",
      " |  __get_pydantic_core_schema__(\n",
      " |      source: 'type[BaseModel]',\n",
      " |      handler: 'GetCoreSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'CoreSchema'\n",
      " |\n",
      " |  __get_pydantic_json_schema__(\n",
      " |      core_schema: 'CoreSchema',\n",
      " |      handler: 'GetJsonSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
      " |      meaning that creating an instance of the class may fail.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by Pydantic.\n",
      " |\n",
      " |      Note:\n",
      " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
      " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
      " |\n",
      " |  __pydantic_on_complete__() -> 'None'\n",
      " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
      " |\n",
      " |      This typically happens when the class is created (just before\n",
      " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
      " |      except when forward annotations are used that could not immediately be resolved.\n",
      " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
      " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>,\n",
      " |      mode: 'JsonSchemaMode' = 'validation',\n",
      " |      *,\n",
      " |      union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of'\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
      " |\n",
      " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
      " |              keyword to combine schemas (the default).\n",
      " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
      " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
      " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
      " |              `any_of`.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(\n",
      " |      *,\n",
      " |      force: 'bool' = False,\n",
      " |      raise_errors: 'bool' = True,\n",
      " |      _parent_namespace_depth: 'int' = 2,\n",
      " |      _types_namespace: 'MappingNamespace | None' = None\n",
      " |  ) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      from_attributes: 'bool | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(\n",
      " |      json_data: 'str | bytes | bytearray',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(\n",
      " |      path: 'str | Path',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(\n",
      " |      b: 'str | bytes',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(\n",
      " |      *,\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_fields = {'content': FieldInfo(annotation=Union[str, NoneType], ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(response.message))\n",
    "help(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "67802868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: assistant\n",
      "content: \n",
      "thinking: The user wants the square of 2. They want only the numeric result, no other explanation. They gave a tool: compute square_number. So we should use the tool. The input should be number: 2. Then we return the result.\n",
      "\n",
      "We need to call the function.\n",
      "images: None\n",
      "tool_name: None\n",
      "tool_calls: [ToolCall(function=Function(name='square_number', arguments={'number': 2}))]\n",
      "length of tool_calls: 1\n"
     ]
    }
   ],
   "source": [
    "# inspect response.message prints\n",
    "print(\"role:\", response.message.role)\n",
    "print(\"content:\", response.message.content)\n",
    "print(\"thinking:\", response.message.thinking)\n",
    "print(\"images:\", response.message.images)\n",
    "print(\"tool_name:\", response.message.tool_name)\n",
    "print(\"tool_calls:\", response.message.tool_calls)\n",
    "print(\"length of tool_calls:\", len(response.message.tool_calls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f0eaf",
   "metadata": {},
   "source": [
    "##### Tools calling alt 1 - END OF DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85007a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50512ebe",
   "metadata": {},
   "source": [
    "#### Tools calling alt 1 - Two tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "517b00b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the message to send to the model and use the tool\n",
    "message = \"\"\"\n",
    "        Compute the square of 2 using the available tool. Return only the numeric result and use it to call another tool to select an item.\n",
    "        Only return the squared number and the item.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user',\n",
    "     'content': message,\n",
    "    }\n",
    "]\n",
    "\n",
    "# # pass tools to the model\n",
    "# response = chat(\n",
    "#     model='gpt-oss:20b',\n",
    "#     messages=messages,\n",
    "#     tools=[square_number, select_item],\n",
    "#     think=\"high\")\n",
    "\n",
    "# messages.append(response.message)\n",
    "\n",
    "# # Debug: internal reasoning (optional)\n",
    "# if hasattr(response.message, \"thinking:\"):\n",
    "#     print(\"thinking:\\n\", response.message.thinking, \"\\n\") # Print thinking process before tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3597381",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [square_number, select_item]\n",
    "\n",
    "while True:\n",
    "    response = chat(\n",
    "        model=\"gpt-oss:20b\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        think=\"high\"\n",
    "    )\n",
    "\n",
    "    message = response.message\n",
    "    messages.append(message)\n",
    "\n",
    "    tool_calls = message.get(\"tool_calls\")\n",
    "\n",
    "    # Stop if no tool calls are requested\n",
    "    if not tool_calls:\n",
    "        break\n",
    "\n",
    "    for call in tool_calls:\n",
    "        tool_name = call[\"function\"][\"name\"]\n",
    "        args = call[\"function\"][\"arguments\"]\n",
    "\n",
    "        if tool_name == \"square_number\":\n",
    "            result = square_number(**args)\n",
    "        elif tool_name == \"select_item\":\n",
    "            result = select_item(**args)\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_name\": tool_name,\n",
    "            \"content\": str(result)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "879169ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this poin tools have been executed\n",
    "# Results are in messages\n",
    "# But there is no final answer yet\n",
    "\n",
    "final_response = chat(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a5fde0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\"Final Answer:\", final_response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aadc27ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(final_response.message.get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9d41b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# print(len(response.message.thinking))\n",
    "print(response.message.thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35a06b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(response.message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "146b7fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# debug prints after adding tool result\n",
    "print(type(messages))\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5b7475b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(final_response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "469c192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.message.tool_calls:\n",
    "  # process each tool call \n",
    "  for call in response.message.tool_calls:\n",
    "    # execute the appropriate tool\n",
    "    if call.function.name == 'square_number':\n",
    "      result = square_number(**call.function.arguments)\n",
    "    elif call.function.name == 'select_item':\n",
    "      result = select_item(**call.function.arguments)\n",
    "    else:\n",
    "      result = 'Unknown tool'\n",
    "    # add the tool result to the messages\n",
    "    messages.append({'role': 'tool',  'tool_name': call.function.name, 'content': str(result)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a461b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# debug prints after adding tool result\n",
    "print(type(messages))\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee8abce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the final response\n",
    "final_response = chat(model='gpt-oss:20b', \n",
    "                      messages=messages, \n",
    "                      tools=[square_number, select_item], \n",
    "                      think=\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcebf31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(final_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4273e",
   "metadata": {},
   "source": [
    "Limitations in Ollama, it cannot really call multiple tools?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5439d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e4e987",
   "metadata": {},
   "source": [
    "### 2. OpenAI SDK\n",
    "\n",
    "### Tools calling alt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35dfb16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library version: 2.8.1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(\"Library version:\", openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ac82e",
   "metadata": {},
   "source": [
    "Ollama exposes a Chat Completion-compatible API, so we can use the OpenAI SDK withouth chaning much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d77b9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Prompt‑Engineering for the **gpt‑oss:20B** Model  \n",
      "*(An 20‑billion‑parameter, open‑source, instruction‑tuned GPT‑style model)*  \n",
      "\n",
      "Below is a practical, step‑by‑step reference that covers everything you need to get **clean, consistent, and usable output** from gpt‑oss:20B. It blends the *science* of prompt design (token limits, instruction tone, etc.) with *hands‑on tips* that you can apply right away, whether you’re calling the model via the 🤗 Transformers API, a custom Docker deployment, or a local web front‑end.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Know the Model’s Key Characteristics\n",
      "\n",
      "| Feature | What You Need to Know |\n",
      "|---------|------------------------|\n",
      "| **Token limit** | 8 192 tokens (≈ 32 k words).  |  \n",
      "| **Best instruction‑style** | A **system message** followed by a **user message** gives the model the clearest direction. |\n",
      "| **Architecture** | LLaMA‑style transformer with full‑attention. No special prompt format like GPT‑3’s “system” vs. “assistant” tags – just a list of messages. |\n",
      "| **Calibration** | It tends to *hallucinate* on uncertain facts.  Keep the output length moderate and add a “source‑check” instruction if needed. |\n",
      "| **Temperature / Top‑p** | Default 0.7, top‑p 0.95 is a safe starting point. Use lower temperature (≤0.3) for deterministic, fact‑based responses; higher temperature for creativity. |\n",
      "| **Fine‑tuning / LoRA** | If you have domain‑specific data, the open‑source framework lets you fine‑tune or use LoRA adapters. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Build a Three‑Layer Prompt Template\n",
      "\n",
      "1. **System Message** – sets *global tone* and *model behaviour*.  \n",
      "2. **User Message** – your *input question or task*.  \n",
      "3. **Assistant Response** – the output the model should produce (you specify the desired format).\n",
      "\n",
      "> **Tip**: Keep the system message concise (≤ 30 tokens).  \n",
      "> **Tip**: The user message should include a *clear question* or *command*, not a vague “tell me about X”.\n",
      "\n",
      "### Example Template\n",
      "\n",
      "```plaintext\n",
      "# System\n",
      "You are a helpful AI that writes concise, fact‑checked answers.  \n",
      "If you’re unsure, say “I’m not sure” instead of making up data.  \n",
      "Use markdown for formatting.\n",
      "\n",
      "# User\n",
      "Explain how to prompt the gpt‑oss:20B model effectively.\n",
      "\n",
      "# Assistant\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Practical Prompt‑Engineering Techniques\n",
      "\n",
      "| Technique | What It Does | How to Use It in gpt‑oss:20B |\n",
      "|-----------|--------------|--------------------------------|\n",
      "| **Few‑Shot Prompting** | Gives the model concrete examples. | Include 1–3 *mini‑examples* that show the structure of a good answer. |\n",
      "| **Chain‑of‑Thought (CoT)** | Encourages the model to reason step‑by‑step. | Start the user message with “First, let’s think step by step:”. |\n",
      "| **Explicit Instructions** | Reduces ambiguity. | Use bullet points in the user message to ask for specific sections. |\n",
      "| **Token‑budgeting** | Saves you from truncation. | Before sending a prompt, count tokens (e.g., with `transformers`’ `get_tokenizer`). |\n",
      "| **Control tokens / Stop tokens** | Limits output length or marks the end. | Add `\"stop\": [\"\\n\"]` or `\"stop\": [\"END\"]` in the generation parameters. |\n",
      "| **Structured JSON Output** | Easy to parse programmatically. | End the system message with “Answer in JSON, no extra keys”. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Step‑by‑Step Example\n",
      "\n",
      "### Goal  \n",
      "Write a concise guide on *prompting gpt‑oss:20B* for a 5‑minute blog post.\n",
      "\n",
      "```plaintext\n",
      "# System\n",
      "You are an expert writer for technology blogs.  Use no more than 300 words.  List the main steps in an ordered list.  Provide one example prompt for each step.\n",
      "\n",
      "# User\n",
      "Create a short guide on how to prompt the gpt‑oss:20B model for a 5‑minute blog post.\n",
      "\n",
      "# Assistant\n",
      "```\n",
      "\n",
      "**Why it works**\n",
      "\n",
      "- **Clear tone** (“expert writer”), **length constraint** (≤ 300 words), and **output format** (ordered list) are all set in the system message.  \n",
      "- The user request is specific and concise.  \n",
      "- The assistant knows exactly what to produce.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Handling Long‑Form Content\n",
      "\n",
      "| Situation | Recommended Approach |\n",
      "|---|---|\n",
      "| You need > 8 192 tokens of content | Split the prompt into **chapters**; feed each chapter separately. |\n",
      "| You need to refer back to earlier text | Provide the *relevant excerpt* (≤ 1 000 tokens) before the new prompt. |\n",
      "| You want a single continuous output | Use the *“repetition‑control”* trick: ask the model to produce a string that ends with “END”, then feed that string back with a new instruction. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Fine‑Tuning / LoRA with gpt‑oss:20B\n",
      "\n",
      "If you own specialized content (e.g., medical notes, legal briefs), you can give your model a *domain voice*:\n",
      "\n",
      "1. **Fine‑tune**:  \n",
      "   ```bash\n",
      "   python -m transformers.train \\\n",
      "     --model_name_or_path huggingface-community/gpt-oss-20b \\\n",
      "     --train_file domain_train.txt \\\n",
      "     --validation_file domain_val.txt \\\n",
      "     --per_device_train_batch_size 1 \\\n",
      "     --gradient_accumulation_steps 8 \\\n",
      "     --max_seq_length 8192 \\\n",
      "     --bf16 \\\n",
      "     --fp16 \\\n",
      "     --output_dir fine_tuned_gpt_oss_20b \\\n",
      "     --learning_rate 2e-5 \\\n",
      "     --num_train_epochs 3\n",
      "   ```\n",
      "\n",
      "2. **LoRA Adapter**:  \n",
      "   - Install `peft` library.  \n",
      "   - Wrap the model in a LoRA adapter, fine‑tune only the adapter weights (fast, low storage).  \n",
      "\n",
      "> **Tip**: When using LoRA, keep `lora_alpha` and `rank` small (e.g., rank = 8) to minimize memory usage.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Common Pitfalls & How to Avoid Them\n",
      "\n",
      "| Pitfall | Fix |\n",
      "|---------|-----|\n",
      "| **Hallucinations** | Add “if uncertain, say ‘I’m not sure’” in the system message; verify facts manually. |\n",
      "| **Truncation** | Use the `tokenizer` to preview prompt size; adjust by removing verbose adjectives. |\n",
      "| **Over‑prompting** | Too many examples or too complex wording lowers generation quality. Keep to 1–3 examples max. |\n",
      "| **Token Overuse by System** | The system message itself consumes tokens; don’t embed large data or verbose policy in it. |\n",
      "| **Output Not Structured** | Explicitly request JSON or markdown; use stop tokens to guard against runaway text. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Example Prompt‑Engineering Playbook\n",
      "\n",
      "| Type of Task | Prompt Template | Parameter Tweaks |\n",
      "|--------------|-----------------|------------------|\n",
      "| **Coding** | \"Write a function in Python that does X. Include doctests.\" | temperature = 0.3, top_p = 1.0, max_new_tokens = 1024 |\n",
      "| **Creative Writing** | \"Draft a short horror story in the style of Shirley Jackson.\" | temperature = 0.9, top_p = 0.95, max_new_tokens = 2048 |\n",
      "| **Summarization** | \"Summarize the following article in 3 bullet points.\" | temperature = 0.2, top_p = 0.9, max_new_tokens = 256 |\n",
      "| **Data Extraction** | \"From the table, extract the sales numbers for Q3.\" | temperature = 0.0, top_p = 1.0, max_new_tokens = 128 |\n",
      "| **Dialogue Generation** | \"Generate a 5‑message conversation between a student and a teacher about…\". | temperature = 0.7, top_p = 0.95, max_new_tokens = 512 |\n",
      "\n",
      "Feel free to tweak `max_new_tokens`, temperature, and top‑p to match the length and creativity you need — the gpt‑oss:20B model is flexible enough to handle most of these scenarios.\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Sample Code (Python + 🤗 Transformers)\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
      "\n",
      "model_name = \"huggingface-community/gpt-oss-20b\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
      "\n",
      "pipe = pipeline(\n",
      "    \"text-generation\",\n",
      "    model=model,\n",
      "    tokenizer=tokenizer,\n",
      "    max_new_tokens=1024,\n",
      "    temperature=0.7,\n",
      "    top_p=0.95,\n",
      "    repetition_penalty=1.2,\n",
      ")\n",
      "\n",
      "system_msg = (\n",
      "    \"You are a helpful AI that writes concise, fact-checked answers. \"\n",
      "    \"If you're unsure, say 'I'm not sure'. Use markdown for formatting.\"\n",
      ")\n",
      "\n",
      "user_msg = (\n",
      "    \"Explain how to prompt the gpt-oss:20B model effectively. \"\n",
      "    \"Provide a three‑step guide, each with an example prompt.\"\n",
      ")\n",
      "\n",
      "prompt = f\"[SYSTEM] {system_msg}\\n\\n[USER] {user_msg}\\n\\n[ASSISTANT]\"\n",
      "output = pipe(prompt)[0][\"generated_text\"]\n",
      "print(output.split(\"[ASSISTANT]\")[-1].strip())\n",
      "```\n",
      "\n",
      "> **Note**: The tags `[SYSTEM]`, `[USER]`, `[ASSISTANT]` are *just for readability*; gpt‑oss:20B treats every line as plain text, but the structure helps you keep the roles clear.\n",
      "\n",
      "---\n",
      "\n",
      "## 10. Final Checklist\n",
      "\n",
      "Before hitting **Generate**, run through this quick sanity scan:\n",
      "\n",
      "1. **Token Count** ≤ 8 192 (prompt + max new tokens).  \n",
      "2. **System** sets tone, length, formatting.  \n",
      "3. **User** is a single, clear instruction or question.  \n",
      "4. **Stop Condition** (optional) to terminate early.  \n",
      "5. **Temperature / Top_p** chosen for the desired creativity.  \n",
      "6. **Verification** of factual claims (especially for critical content).\n",
      "\n",
      "If everything checks out, you’re ready to let gpt‑oss:20B generate content that feels polished, on‑topic, and reliably useful.\n",
      "\n",
      "--- \n",
      "\n",
      "**Happy prompting!**  \n",
      "\n",
      "*(You can further experiment with the OpenAI “Chat” style API wrapper that takes a list of messages; just substitute the `[SYSTEM]` and `[USER]` lines with the appropriate OpenAI JSON objects.)*\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Local Ollama API\n",
    "    api_key=\"ollama\"                       # Dummy key\n",
    ")\n",
    " \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain how to prompt the gpt-oss:20b model effectively.\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b14270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the above... TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e626950",
   "metadata": {},
   "source": [
    "### 3. Ollama + Python agent loop.  \n",
    "Fully local, minimal dependencies, deterministic.  \n",
    "We must code the orchestration ourselves.\n",
    "\n",
    "### Tools calling alt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d455e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out below ... TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb641c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools.py\n",
    "\n",
    "def square_number(number: int) -> int:\n",
    "    \"\"\"Compute the square of a number.\"\"\"\n",
    "    if not isinstance(number, int):\n",
    "        raise ValueError(\"number must be an integer.\")\n",
    "    return number * number\n",
    "\n",
    "\n",
    "def select_item(index: int) -> str:\n",
    "    \"\"\"Select an item from a list based on index.\"\"\"\n",
    "    items = ['BILLY', 'PAX', 'MARKUS', 'MICKE', 'ALGOT']\n",
    "    if index < 1 or index > len(items):\n",
    "        raise ValueError(\"Index out of range\")\n",
    "    return items[index - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5089f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answer: 4, None\n"
     ]
    }
   ],
   "source": [
    "### TRY THIS - Download ministral model!\n",
    "\n",
    "# agent_loop.py\n",
    "\n",
    "from ollama import chat\n",
    "#from tools import square_number, select_item\n",
    "\n",
    "# Register tools\n",
    "TOOLS = {\n",
    "    \"square_number\": square_number,\n",
    "    \"select_item\": select_item\n",
    "}\n",
    "\n",
    "# Initial user prompt\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": (\n",
    "        \"Compute the square of 2. \"\n",
    "        \"Then use that number to select an item from the list. \"\n",
    "        \"Return only the squared number and the selected item.\"\n",
    "    )\n",
    "}]\n",
    "\n",
    "MAX_STEPS = 5  # safety to prevent infinite loops\n",
    "\n",
    "for step in range(MAX_STEPS):\n",
    "    # Step 1: Ask the model what to do next\n",
    "    response = chat(\n",
    "        model=\"ministral-3:14b\", # This is preferred for tool use\n",
    "        messages=messages,\n",
    "        tools=list(TOOLS.values())  # Ollama knows the available functions\n",
    "    )\n",
    "\n",
    "    assistant_msg = response.message\n",
    "    messages.append(assistant_msg)\n",
    "\n",
    "    # Step 2: Check for tool calls\n",
    "    tool_calls = assistant_msg.get(\"tool_calls\", [])\n",
    "    if not tool_calls:\n",
    "        break  # no more tools to execute\n",
    "\n",
    "    # Step 3: Execute each tool call\n",
    "    for call in tool_calls:\n",
    "        name = call[\"function\"][\"name\"]\n",
    "        args = call[\"function\"][\"arguments\"]\n",
    "\n",
    "        if name not in TOOLS:\n",
    "            raise RuntimeError(f\"Unknown tool: {name}\")\n",
    "\n",
    "        # Execute tool\n",
    "        result = TOOLS[name](**args)\n",
    "\n",
    "        # Feed result back into conversation\n",
    "        messages.append({\n",
    "            \"role\": \"tool\",\n",
    "            \"tool_name\": name,\n",
    "            \"content\": str(result)\n",
    "        })\n",
    "\n",
    "# Step 4: Force final answer generation\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Provide the final answer using the tool results only.\"\n",
    "})\n",
    "\n",
    "final_response = chat(\n",
    "    model=\"ministral-3:14b\", # This is preferred for tool use\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"Final Answer:\", final_response.message[\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125056a2",
   "metadata": {},
   "source": [
    "### Realistic solutions: ###\n",
    "\n",
    "1. Switch to a more instruction-tuned local model like `ministral-3:14b`.  \n",
    "  * It reliably follows structured instructions and JSON output.\n",
    "  * Fits your RTX 5090 VRAM with 4/8-bit quantization.\n",
    "2. Simpler agent loop (without JSON) works better with `gpt-oss:20b`:\n",
    "  * Ask the model explicitly for the next tool and arguments in plain text.\n",
    "  * Python parses it with simple string parsing (e.g., regex for numbers) instead of strict JSON.  \n",
    "3. If you insist on JSON:\n",
    "  * You may need a “validator + retry” loop: keep prompting the model until it outputs valid JSON.\n",
    "  * `gpt-oss:20b` may fail 30–50% of the time on the first attempt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
