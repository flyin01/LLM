{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360370ee",
   "metadata": {},
   "source": [
    "## Ollama local LLMs  \n",
    "\n",
    "This notebook is using Ollama and the following for running local LLMs:  \n",
    "* ollama sdk\n",
    "* openai sdk \n",
    "  \n",
    "Source 1: [Ollama Docs](https://docs.ollama.com/capabilities/tool-calling#python-2)  \n",
    "Source 2: [OpenaAI Cookbook](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)  \n",
    "  \n",
    "**Setup:**  \n",
    "1. To load a local model run the following from the terminal: `ollama run gpt-oss:20b`\n",
    "2. Run the notebook using venv and install libs from `requirements.txt`\n",
    "  \n",
    "### 1. Ollama SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9882de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\repo\\LLM\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b82d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '/requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "# !pip install -r /requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9985ccf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.13.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45e8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa1bb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Propose the optimal prompt for the gpt-oss:20b model.\n",
      "        Give an example of a poorly constructed prompt and improve it until it is optiomal.\n",
      "        The prompt should be usefull for agentic tasks with tool calling.\n",
      "        The response should be in markdown format.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model\n",
    "message = \"\"\"\n",
    "        Propose the optimal prompt for the gpt-oss:20b model.\n",
    "        Give an example of a poorly constructed prompt and improve it until it is optiomal.\n",
    "        The prompt should be usefull for agentic tasks with tool calling.\n",
    "        The response should be in markdown format.\n",
    "        \"\"\"\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427369d5",
   "metadata": {},
   "source": [
    "#### Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519cf89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: \n",
      " We need to propose an optimal prompt for gpt-oss:20b model. Provide an example of a poorly constructed prompt and improve it until optimal. The prompt should be useful for agentic tasks with tool calling. The response should be in markdown.\n",
      "\n",
      "We need to consider the gpt-oss:20b model: it's a 20-billion-parameter open-source model, presumably from the GPT-OSS family. We need to propose an optimal prompt: likely a structured prompt that encourages the model to act as an agent, making use of tool calling, and making sure it's concise, gives context, instructions, and clarifies tool usage. We can propose a prompt format: include system messages, user messages, clarifying that the model should think step by step, use tool calls. For GPT-OSS, tool calling support might be via \"function calls\" interface or \"tool calls\" similar to GPT-4.\n",
      "\n",
      "We should provide an example of a poorly constructed prompt, then show improved versions.\n",
      "\n",
      "We should respond in markdown format.\n",
      "\n",
      "We need to consider the difference between GPT-4 and GPT-OSS 20B: the 20B model is less powerful, but still large. Prompt should guide it. Might use \"You are an agent\" type of instructions. Also incorporate role and context. We should propose a structured approach: system: sets role, guidelines. Then maybe a user prompt: includes the task, a short context, and maybe tool list. The prompt should include a \"Tool calling\" instructions: e.g., \"When you need to fetch data, use the tool 'search' with arguments, etc.\"\n",
      "\n",
      "We might mention to use a \"tool\" format: \"CALL TOOL: tool_name(...)\" etc. In GPT-4 function calling, it's a structured JSON. For GPT-OSS, maybe tool calls are not natively supported, but we can emulate via instructions.\n",
      "\n",
      "We can propose a template like:\n",
      "\n",
      "```\n",
      "System: [role, guidelines, instruction on step-by-step reasoning, tool calling, etc.]\n",
      "\n",
      "User: [task description, context, list of available tools with signatures, instructions on how to call them]\n",
      "\n",
      "Assistant: [start of response: first step, maybe \"I will first think about the next step\", etc.]\n",
      "```\n",
      "\n",
      "But we want a single prompt that can be used by an agent to get better results. Possibly the prompt should include a small code snippet that sets up the environment for tool calling. However, as the prompt is to be used by the model, we cannot include code; we can instruct the model to produce certain formatted output.\n",
      "\n",
      "We might provide a \"tool call\" syntax: For example:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": \"search\",\n",
      "  \"arguments\": {\n",
      "    \"query\": \"example\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "The model will output that, and the orchestrator will interpret and call the tool.\n",
      "\n",
      "Alternatively, we can give a simple instruction: \"When you need to call a tool, output in JSON format: { tool: 'tool_name', arguments: { ... } }\".\n",
      "\n",
      "But for GPT-OSS:20b, tool calling might not be built-in, but the user can parse the output. So we can define the syntax.\n",
      "\n",
      "We also need to propose an example of a poorly constructed prompt and then refine it step by step.\n",
      "\n",
      "We'll produce a markdown answer:\n",
      "\n",
      "- Title: Optimal Prompt for gpt-oss:20b for Agentic Tool-Calling Tasks\n",
      "- Section: Overview\n",
      "- Section: Optimal Prompt Template\n",
      "- Section: Example: Poorly Constructed Prompt\n",
      "- Section: Step-by-Step Refinement (three iterations)\n",
      "- Section: Final Optimized Prompt\n",
      "- Additional notes.\n",
      "\n",
      "Make sure to emphasize the important parts: role specification, constraints, step-by-step reasoning, calling tool with proper syntax, context, clarity, limiting verbosity.\n",
      "\n",
      "We can also mention that we should keep the prompt size moderate to not exceed token budget; but for GPT-20B, prompt token limit may be ~8k or 32k tokens? We need to be aware. Usually GPT-3.5/4 have 8k or 32k. GPT-OSS:20b may have 8k. We can keep the prompt under 512 tokens.\n",
      "\n",
      "We can also mention the best practice of providing the tool list as part of the system message or user prompt: e.g., \"You have access to the following tools: search (searches the web), calculate (performs arithmetic), etc.\"\n",
      "\n",
      "Let's propose the optimal prompt:\n",
      "\n",
      "```\n",
      "**System Message**:\n",
      "You are a sophisticated language model trained to act as an autonomous agent. Your role is to assist the user by performing complex tasks using a set of tools. You must:\n",
      "1. Think carefully and plan your steps before taking action.\n",
      "2. Use the available tools only when necessary.\n",
      "3. Output tool calls in JSON format: { \"name\": \"tool_name\", \"arguments\": { ... } }.\n",
      "4. When a tool call completes, you will receive the tool's output as the next message. Use it to inform your next step.\n",
      "5. If no tool is needed, answer directly.\n",
      "6. Keep responses concise but complete.\n",
      "\n",
      "**Available Tools**:\n",
      "- `search`: arguments: `{\"query\": string}`; returns a string summary of top results.\n",
      "- `calculate`: arguments: `{\"expression\": string}`; returns the result as a string.\n",
      "- `translate`: arguments: `{\"text\": string, \"target_language\": string}`; returns the translation.\n",
      "- `summarize`: arguments: `{\"text\": string}`; returns a concise summary.\n",
      "\n",
      "**User Prompt**:\n",
      "[Insert user's task description and any context. Encourage the user to be specific about the desired outcome.]\n",
      "\n",
      "**Assistant**:\n",
      "(Here you begin your response, following the guidelines.)\n",
      "```\n",
      "\n",
      "But we need to propose the prompt that is passed to the model. Usually, you pass a message list with role: \"system\" and role: \"user\". So the prompt may be a combined system message and user message. But the question: \"Propose the optimal prompt for the gpt-oss:20b model.\" So we need to provide the actual prompt string, possibly as a \"system\" message that defines the guidelines, and then a \"user\" message that contains the task.\n",
      "\n",
      "However, we can produce a template that includes placeholders for the user input. Eg:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"role\": \"system\",\n",
      "  \"content\": \"... guidelines ...\",\n",
      "  \"tools\": [ ... ]\n",
      "}\n",
      "{\n",
      "  \"role\": \"user\",\n",
      "  \"content\": \"Task: ... Context: ...\"\n",
      "}\n",
      "```\n",
      "\n",
      "We should also show how to incorporate tool calling syntax.\n",
      "\n",
      "But we might also propose to use a single prompt with an instruction like: \"You are an agent. When you need to use a tool, output JSON like ...\". So we can propose a \"system\" message that instructs the model. The user message is then the task.\n",
      "\n",
      "Better to give a specific example: The prompt for the user might be: \"Find the top 5 companies with the highest market cap in 2023, and provide a summary.\" The model will think, decide to use search, produce tool call, then handle response.\n",
      "\n",
      "But the prompt we propose is generic, so the user can fill in the task. The prompt should instruct the model to be agentic and call tools accordingly.\n",
      "\n",
      "We should also propose a best practice: keep the prompt concise, but provide a clear structure for the model to parse. Use bullet lists for guidelines. Use examples.\n",
      "\n",
      "We also need to give an example of a poorly constructed prompt, then gradually improve it. For each iteration, show the prompt and explain why it's better.\n",
      "\n",
      "Potential poor prompts:\n",
      "\n",
      "- \"Help me find the best hotels in Paris.\" (too vague, no context, no instructions about tool usage)\n",
      "- \"Search for hotels.\" (lack of instruction on output formatting)\n",
      "- \"Write code.\" (lack of structure)\n",
      "- \"You are GPT, you can call search.\" (no guidelines, no step-by-step)\n",
      "\n",
      "We can refine to:\n",
      "\n",
      "- \"You are a helpful assistant. Use the search tool to find the best hotels in Paris and return them.\" (some improvement)\n",
      "- Then add step-by-step: \"Think first about next step. If you need to call a tool, output JSON.\"\n",
      "\n",
      "We can continue until it's optimal.\n",
      "\n",
      "We need to produce final answer in markdown. Provide the initial poor prompt, then improvements with numbered bullet points or steps.\n",
      "\n",
      "Let's structure the answer:\n",
      "\n",
      "1. Title: Optimal Prompt for GPT-OSS 20B with Tool Calling\n",
      "2. Section: Why a well-structured prompt matters\n",
      "3. Section: Optimal Prompt Template\n",
      "4. Section: Example of a poorly constructed prompt\n",
      "5. Section: Iterative refinement of the prompt\n",
      "   - Version 1 (Poor)\n",
      "   - Version 2 (First improvement)\n",
      "   - Version 3 (Second improvement)\n",
      "   - Version 4 (Third improvement)\n",
      "   - Version 5 (Final)\n",
      "6. Section: Final optimized prompt\n",
      "7. Section: Additional guidelines for use\n",
      "\n",
      "We need to show the prompts as code blocks in markdown.\n",
      "\n",
      "Also, we should include an explanation of each step of improvement: e.g., adding role, step-by-step reasoning, tool call format, context, etc.\n",
      "\n",
      "Also, maybe mention that the prompt includes the tool signature, so the model knows what arguments to provide.\n",
      "\n",
      "Additionally, maybe mention that the prompt should be short enough to fit within token limits. So we need to be careful about length.\n",
      "\n",
      "We can propose the prompt as two parts: system message (guidelines) and user message (task). The system message can include the tool list. The user message can be a placeholder.\n",
      "\n",
      "We also can provide a single combined prompt string that can be passed as a single message with role: \"system\" (if using new chat format). But we can propose the system message only.\n",
      "\n",
      "Let's think about the typical format for ChatGPT: messages: system, user, assistant. The system message sets the overall behavior. The user message is the user's request. So the prompt we propose will be the system message. Then the user provides the task. So we can give a system message that covers everything. Or we can propose that the system message includes tool definitions.\n",
      "\n",
      "The system message might be:\n",
      "\n",
      "```\n",
      "You are an AI assistant designed to act autonomously and use available tools to accomplish user tasks. Follow these rules:\n",
      "1. Think through the task step by step before acting.\n",
      "2. Only use a tool when necessary. Output a JSON object with the format { \"name\": \"<tool>\", \"arguments\": { ... } } for tool calls. No other output is allowed until the tool returns a response.\n",
      "3. Upon receiving a tool's response, incorporate it and continue. \n",
      "4. If no tool is needed, answer directly in plain text.\n",
      "5. Keep all responses concise but complete.\n",
      "Available tools:\n",
      "- search (args: { query: string }) returns a summary of top results.\n",
      "- calculate (args: { expression: string }) returns the result.\n",
      "- translate (args: { text: string, target_language: string }) returns translation.\n",
      "- summarize (args: { text: string }) returns concise summary.\n",
      "User's request: <placeholder>\n",
      "```\n",
      "\n",
      "We might mention that the user can just type the request, and the system message will fill in the rest. But the system message may be static except for the placeholder.\n",
      "\n",
      "Alternatively, we can propose a single prompt that includes the entire conversation. But it's better to separate system message from user message.\n",
      "\n",
      "Thus, the \"optimal prompt\" might be a system message plus a user message template. We can propose a single string that includes placeholders. For instance:\n",
      "\n",
      "```\n",
      "SYSTEM:\n",
      "You are an autonomous agent...\n",
      "TOOLS...\n",
      "When a tool is required, output JSON ...\n",
      "...\n",
      "\n",
      "USER:\n",
      "{user_query}\n",
      "```\n",
      "\n",
      "But for GPT-OSS:20b, maybe it's just a chat prompt with one message. But we can show as a multi-message array.\n",
      "\n",
      "Ok, let's produce a full answer. We'll write:\n",
      "\n",
      "```\n",
      "# Optimal Prompt for GPT-OSS:20b (Agentic Tasks with Tool Calling)\n",
      "\n",
      "## 1. Why a well‑structured prompt is critical\n",
      "...\n",
      "```\n",
      "\n",
      "Then provide the template.\n",
      "\n",
      "In the improvement section, show each iteration and why it's better.\n",
      "\n",
      "We must use markdown formatting.\n",
      "\n",
      "Now, think about how to illustrate the \"tool call\" output. We might show the model's response:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"name\": \"search\",\n",
      "  \"arguments\": {\n",
      "    \"query\": \"best hotels in Paris 2024\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Then after the tool returns:\n",
      "\n",
      "```\n",
      "Top 5 hotels in Paris: ... \n",
      "```\n",
      "\n",
      "But the prompt itself needs to instruct the model on the format. So we must include the output format.\n",
      "\n",
      "Also mention that the model should not include extra text before or after the JSON; otherwise the orchestrator might misinterpret. So we instruct \"only output the JSON\" etc.\n",
      "\n",
      "Also mention that the model should handle errors and fallback to a direct answer if tool fails.\n",
      "\n",
      "Also mention that the model should consider privacy, e.g., avoid disclosing personal data, etc. But that might be optional.\n",
      "\n",
      "We can also mention that the prompt includes the \"role\" and \"function\" of each tool, so the model can plan.\n",
      "\n",
      "We should also note that the prompt may contain \"You are the assistant\" to help model.\n",
      "\n",
      "Ok, let's outline the answer:\n",
      "\n",
      "1. Title\n",
      "2. Explanation\n",
      "3. Optimal Prompt Template\n",
      "4. Example of Poor Prompt\n",
      "5. Iterative refinement\n",
      "6. Final Prompt\n",
      "7. Additional Tips\n",
      "\n",
      "Within the refinement, we can show:\n",
      "\n",
      "- Poor: \"Find best hotels in Paris\"\n",
      "- First improvement: \"Help me find best hotels in Paris.\"\n",
      "- Second improvement: \"I need the top 5 hotels in Paris for July 2024.\"\n",
      "- Third improvement: \"Use the search tool. Output JSON. Etc.\"\n",
      "\n",
      "We should also highlight the steps: adding tool list, JSON output, step-by-step, context, instructions for the model.\n",
      "\n",
      "Ok, let's write.\n",
      "\n",
      "We'll produce a markdown answer with code blocks for each prompt.\n",
      "\n",
      "We'll also include the final prompt in a code block.\n",
      "\n",
      "Also note that GPT-OSS:20b might not have \"function calling\" but we can instruct the model to produce the JSON, and the orchestrator can parse.\n",
      "\n",
      "Also mention that the prompt should keep tokens low, so we don't include huge tool descriptions.\n",
      "\n",
      "Ok, let's write the answer. \n",
      "\n",
      "Answer: \n",
      " # Optimal Prompt for **gpt‑oss:20b** – Agentic Tasks with Tool Calling  \n",
      "> **Goal:** Create a single, reusable prompt that turns GPT‑OSS 20B into a reliable autonomous agent that can think step‑by‑step, decide when to call tools, and format tool calls in a machine‑readable way.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Why a Well‑Structured Prompt Matters  \n",
      "\n",
      "| Challenge | Impact | How a Good Prompt Helps |\n",
      "|-----------|--------|------------------------|\n",
      "| **Ambiguous instructions** | The model may guess, producing irrelevant or noisy output. | Explicit rules (`think first`, `output JSON`, `only JSON when calling a tool`) remove guesswork. |\n",
      "| **Tool misuse or missing arguments** | Tool calls fail or return incomplete data. | Providing a *tool signature* in the prompt lets the model know exactly what arguments to supply. |\n",
      "| **Excess verbosity** | Tokens are wasted, hitting limits and lowering response quality. | Concise, bullet‑pointed guidelines keep the prompt short. |\n",
      "| **Inconsistent format** | The orchestrator cannot parse the response. | A strict “only JSON” rule guarantees parseability. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. The Optimal Prompt Template  \n",
      "\n",
      "> **Structure**: Two messages – a *system* message that sets the behavior and tool list, and a *user* message that contains the actual request.  \n",
      "> **Length**: < 300 tokens (≈ 150 words).  \n",
      "> **Tool Call Format** (JSON, no surrounding text):  \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"<tool_name>\",\n",
      "  \"arguments\": { ... }\n",
      "}\n",
      "```\n",
      "\n",
      "### System Message (≈ 150 words)\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "### User Message (Insert the actual task)\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "> *When you send the prompt to the model, replace the user message with the real request.*\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Example of a Poorly Constructed Prompt  \n",
      "\n",
      "| Prompt | Why It’s Poor |\n",
      "|--------|---------------|\n",
      "| `\"Help me find the best hotels in Paris.\"` | 1. Too vague – no date, no tool specification. 2. No instruction on JSON format. 3. The model might output plain text, breaking the orchestrator. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Iterative Refinement of the Prompt  \n",
      "\n",
      "| Iteration | Prompt | What Changed | Reason |\n",
      "|-----------|--------|--------------|--------|\n",
      "| **V1 – Basic** | `\"Find the best hotels in Paris.\"` | Added minimal detail. | Still ambiguous; no tool usage guidance. |\n",
      "| **V2 – Adding Context** | `\"Find the best hotels in Paris for July 2024.\"` | Date added. | Better specificity, but still no tool instructions. |\n",
      "| **V3 – Specifying Tool Call** | `\"Use the search tool to find the best hotels in Paris for July 2024. Return the top 5 hotels with a short description.\"` | Explicitly instructs tool usage, but no JSON format. | Good start, but tool output might still be wrapped in prose. |\n",
      "| **V4 – Enforcing JSON Format** | `\"Please call the `search` tool with query: 'top hotels in Paris July 2024'. Return only the JSON object: `{ \"name\": \"search\", \"arguments\": { \"query\": \"top hotels in Paris July 2024\" } }`. No other text.\"` | Explicit JSON format and tool name. | Ensures parseability but now too mechanical; missing planning step. |\n",
      "| **V5 – Full System Prompt** | *The system message described in Section 2.* | Adds planning, error handling, concise answers, and a clean tool signature. | Balances flexibility (the model can decide to plan) with strict output formatting. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Final Optimized Prompt  \n",
      "\n",
      "> **Copy‑paste this as the *system* message** (the first element of the chat).  \n",
      "> **Then provide the user’s request as the second message**.\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "User message (example):\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick Reference Cheat‑Sheet  \n",
      "\n",
      "| Step | What the Model Does | What It Outputs |\n",
      "|------|---------------------|-----------------|\n",
      "| **Planning** | \"I need to decide what tool to use.\" | None (internal thought) |\n",
      "| **Tool Call** | Calls `search`. | JSON object only, e.g. `{\"name\":\"search\",\"arguments\":{\"query\":\"top hotels in Paris July 2024\"}}` |\n",
      "| **Tool Result Received** | Processes text, decides next action. | If no tool needed: plain text answer. If more tools needed: another JSON. |\n",
      "| **Error** | Detects tool error. | Plain text explanation, optionally new plan. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Additional Tips  \n",
      "\n",
      "1. **Keep the system message short** – If you hit the token limit, trim verbose explanations.  \n",
      "2. **Avoid unnecessary words** – GPT‑OSS 20B is sensitive to prompt length.  \n",
      "3. **Test with a sandbox** – Simulate tool responses to see how the model reacts before deploying.  \n",
      "4. **Update tool list as needed** – Add new tools by inserting rows in the table; the model will automatically know the signature.  \n",
      "5. **Use a prompt manager** – Store the system message in a constant so you never forget the formatting rules.  \n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought  \n",
      "\n",
      "With this prompt, GPT‑OSS 20B behaves like a disciplined, tool‑using agent: it plans, calls the right tool with the right arguments, and hands back clean, parseable JSON that your orchestrator can act on. This reduces error rates, saves tokens, and gives you a robust foundation for building sophisticated agentic workflows.\n"
     ]
    }
   ],
   "source": [
    "# Define the chat interaction with the model. For tool calling, need think=True\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=[{'role': 'user',\n",
    "               'content':message}],\n",
    "               think = \"high\", #True/False. Except for gpt-oss use levels: \"low\", \"medium\", \"high\"\n",
    "               stream=False\n",
    ")\n",
    "\n",
    "print('Thinking: \\n', response.message.thinking, '\\n')\n",
    "print('Answer: \\n', response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6483fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Optimal Prompt for **gpt‑oss:20b** – Agentic Tasks with Tool Calling  \n",
      "> **Goal:** Create a single, reusable prompt that turns GPT‑OSS 20B into a reliable autonomous agent that can think step‑by‑step, decide when to call tools, and format tool calls in a machine‑readable way.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Why a Well‑Structured Prompt Matters  \n",
      "\n",
      "| Challenge | Impact | How a Good Prompt Helps |\n",
      "|-----------|--------|------------------------|\n",
      "| **Ambiguous instructions** | The model may guess, producing irrelevant or noisy output. | Explicit rules (`think first`, `output JSON`, `only JSON when calling a tool`) remove guesswork. |\n",
      "| **Tool misuse or missing arguments** | Tool calls fail or return incomplete data. | Providing a *tool signature* in the prompt lets the model know exactly what arguments to supply. |\n",
      "| **Excess verbosity** | Tokens are wasted, hitting limits and lowering response quality. | Concise, bullet‑pointed guidelines keep the prompt short. |\n",
      "| **Inconsistent format** | The orchestrator cannot parse the response. | A strict “only JSON” rule guarantees parseability. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. The Optimal Prompt Template  \n",
      "\n",
      "> **Structure**: Two messages – a *system* message that sets the behavior and tool list, and a *user* message that contains the actual request.  \n",
      "> **Length**: < 300 tokens (≈ 150 words).  \n",
      "> **Tool Call Format** (JSON, no surrounding text):  \n",
      "\n",
      "```json\n",
      "{\n",
      "  \"name\": \"<tool_name>\",\n",
      "  \"arguments\": { ... }\n",
      "}\n",
      "```\n",
      "\n",
      "### System Message (≈ 150 words)\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "### User Message (Insert the actual task)\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "> *When you send the prompt to the model, replace the user message with the real request.*\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Example of a Poorly Constructed Prompt  \n",
      "\n",
      "| Prompt | Why It’s Poor |\n",
      "|--------|---------------|\n",
      "| `\"Help me find the best hotels in Paris.\"` | 1. Too vague – no date, no tool specification. 2. No instruction on JSON format. 3. The model might output plain text, breaking the orchestrator. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Iterative Refinement of the Prompt  \n",
      "\n",
      "| Iteration | Prompt | What Changed | Reason |\n",
      "|-----------|--------|--------------|--------|\n",
      "| **V1 – Basic** | `\"Find the best hotels in Paris.\"` | Added minimal detail. | Still ambiguous; no tool usage guidance. |\n",
      "| **V2 – Adding Context** | `\"Find the best hotels in Paris for July 2024.\"` | Date added. | Better specificity, but still no tool instructions. |\n",
      "| **V3 – Specifying Tool Call** | `\"Use the search tool to find the best hotels in Paris for July 2024. Return the top 5 hotels with a short description.\"` | Explicitly instructs tool usage, but no JSON format. | Good start, but tool output might still be wrapped in prose. |\n",
      "| **V4 – Enforcing JSON Format** | `\"Please call the `search` tool with query: 'top hotels in Paris July 2024'. Return only the JSON object: `{ \"name\": \"search\", \"arguments\": { \"query\": \"top hotels in Paris July 2024\" } }`. No other text.\"` | Explicit JSON format and tool name. | Ensures parseability but now too mechanical; missing planning step. |\n",
      "| **V5 – Full System Prompt** | *The system message described in Section 2.* | Adds planning, error handling, concise answers, and a clean tool signature. | Balances flexibility (the model can decide to plan) with strict output formatting. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Final Optimized Prompt  \n",
      "\n",
      "> **Copy‑paste this as the *system* message** (the first element of the chat).  \n",
      "> **Then provide the user’s request as the second message**.\n",
      "\n",
      "```markdown\n",
      "**You are an autonomous AI agent.**  \n",
      "Your job is to help the user accomplish the given task, using a set of pre‑defined tools when necessary. Follow these rules:\n",
      "\n",
      "1. **Plan first** – Think through the next step before doing anything.  \n",
      "2. **Use tools only when needed** – When a tool is required, output **exactly one JSON object** in the format `{ \"name\": \"tool_name\", \"arguments\": { … } }`.  \n",
      "3. **No other output** – Do not add explanation or text before/after the JSON. The orchestrator will parse the JSON and call the tool.  \n",
      "4. **When a tool returns** – Use the result to decide the next step; again output a JSON if another tool is needed, otherwise answer directly in plain text.  \n",
      "5. **Keep responses concise** – Aim for 1–2 sentences per step.  \n",
      "6. **Error handling** – If a tool returns an error, explain the issue in plain text and suggest an alternative or ask for clarification.  \n",
      "\n",
      "**Available Tools**\n",
      "\n",
      "| Tool | Arguments | Description |\n",
      "|------|-----------|-------------|\n",
      "| `search` | `{\"query\": string}` | Search the web and return a short summary of top results. |\n",
      "| `calculate` | `{\"expression\": string}` | Compute the result of a mathematical expression. |\n",
      "| `translate` | `{\"text\": string, \"target_language\": string}` | Translate text to the specified language. |\n",
      "| `summarize` | `{\"text\": string}` | Produce a concise summary of the provided text. |\n",
      "\n",
      "**User’s request** will follow after this message.\n",
      "```\n",
      "\n",
      "User message (example):\n",
      "\n",
      "```markdown\n",
      "Please find the top 5 hotels in Paris for July 2024 and provide a brief summary of each.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Quick Reference Cheat‑Sheet  \n",
      "\n",
      "| Step | What the Model Does | What It Outputs |\n",
      "|------|---------------------|-----------------|\n",
      "| **Planning** | \"I need to decide what tool to use.\" | None (internal thought) |\n",
      "| **Tool Call** | Calls `search`. | JSON object only, e.g. `{\"name\":\"search\",\"arguments\":{\"query\":\"top hotels in Paris July 2024\"}}` |\n",
      "| **Tool Result Received** | Processes text, decides next action. | If no tool needed: plain text answer. If more tools needed: another JSON. |\n",
      "| **Error** | Detects tool error. | Plain text explanation, optionally new plan. |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Additional Tips  \n",
      "\n",
      "1. **Keep the system message short** – If you hit the token limit, trim verbose explanations.  \n",
      "2. **Avoid unnecessary words** – GPT‑OSS 20B is sensitive to prompt length.  \n",
      "3. **Test with a sandbox** – Simulate tool responses to see how the model reacts before deploying.  \n",
      "4. **Update tool list as needed** – Add new tools by inserting rows in the table; the model will automatically know the signature.  \n",
      "5. **Use a prompt manager** – Store the system message in a constant so you never forget the formatting rules.  \n",
      "\n",
      "---\n",
      "\n",
      "### Final Thought  \n",
      "\n",
      "With this prompt, GPT‑OSS 20B behaves like a disciplined, tool‑using agent: it plans, calls the right tool with the right arguments, and hands back clean, parseable JSON that your orchestrator can act on. This reduces error rates, saves tokens, and gives you a robust foundation for building sophisticated agentic workflows.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dec4a",
   "metadata": {},
   "source": [
    "#### Define Tools for LLM\n",
    "What improves tool usage accuracy:\n",
    "1. Ranked by importance:\n",
    "2. Clear parameter names\n",
    "3. Clear docstring\n",
    "4. Simple signature\n",
    "5. Stable return format (JSON-serializable)\n",
    "6. Clean error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5532f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tool 1 - Improved according to above\n",
    "def square_number(number: int) -> int:\n",
    "    \"\"\"Compute the square of a single integer.\n",
    "    Args:\n",
    "        number (int): An integer value to be squared.\n",
    "\n",
    "    Returns:\n",
    "        result (int): The square of the input number.\n",
    "    \"\"\"\n",
    "    if not isinstance(number, int):\n",
    "        raise ValueError(\"number must be an integer.\")\n",
    "    result = number * number\n",
    "    \n",
    "    return result\n",
    "\n",
    "square_number(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79ecfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BILLY'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tool 2 - IMPROVE THIS FUNCTION - TBD\n",
    "def select_item(number: int) -> str:\n",
    "    \"\"\"This function returns an item (str) from a list of items based on the number provided.\n",
    "    Args:\n",
    "        number (int): The the number is the index of the item to be selected from a list of items.\n",
    "\n",
    "    Returns:\n",
    "        item (str): The selected item from the list.    \n",
    "    \"\"\"\n",
    "    \n",
    "    items: list[str] = ['BILLY', 'PAX', 'MARKUS', 'MICKE', 'ALGOT']\n",
    "    if number <= 0 or number > len(items):\n",
    "        return \"Error: Number out of range.\"\n",
    "      \n",
    "    else:\n",
    "        item = items[number-1]\n",
    "    \n",
    "    return item\n",
    "\n",
    "select_item(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a436e",
   "metadata": {},
   "source": [
    "#### Tools calling alt 1 - One tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0170d6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking:\n",
      " We are ChatGPT, with knowledge of available tools. The user says \"Compute the square of 2 using the available tool? Only return the numeric result.\" So we must use the function square_number. There's a tool defined. We need to call the function. The response should be numeric only.\n",
      "\n",
      "We must use the functions namespace: functions.square_number. We'll call with number: 2. The result will be something like {result: ...} but we want only numeric result.\n",
      "\n",
      "We need to produce the function call. The spec says: \"Only return the numeric result.\" However, to use the tool we must produce a JSON function call as the output, I think.\n",
      "\n",
      "But the instructions: \"You are ChatGPT, ... Only return the numeric result.\" Possibly we must directly call the function and output the numeric result. But we can't just call function. The system expects us to produce a tool invocation. Let's think: The instruction says \"Compute the square of 2 using the available tool? Only return the numeric result.\" That suggests we should call the tool and return the numeric result. However the policy for function calls is that we must output a JSON object specifying the tool name and arguments. The tool will then return a result. Then we can output the final answer.\n",
      "\n",
      "But user says only return numeric result. But we have to call the tool. Perhaps we should call the tool and then respond with numeric result. But we can't produce the tool invocation and the result simultaneously. The user expects final output numeric. The system may allow a function call followed by a final output. But typical patterns: we produce a function call, then the tool returns result. Then we produce final. The final should be numeric.\n",
      "\n",
      "But the user instruction says \"Only return the numeric result.\" That means we should not output any additional text, just the number. So maybe we can call the tool and then produce just \"4\" as output. But we cannot do that because we need to call the tool. We may need to output the function call as part of conversation. Usually the system will handle tool calls. The assistant first outputs a tool call. Then the system will run it and provide the output. Then the assistant can respond with final answer.\n",
      "\n",
      "Thus we need to output a JSON to call the function. Then after function result, we need to respond with numeric result only.\n",
      "\n",
      "But the user might want the final numeric answer only. So we might do:\n",
      "\n",
      "Assistant: call function\n",
      "\n",
      "Assistant: 4\n",
      "\n",
      "But that might break the protocol.\n",
      "\n",
      "Given the guidelines, we should produce a function call. Then the system will run it. Then we can output the numeric result.\n",
      "\n",
      "Thus our first assistant message: a JSON object:\n",
      "\n",
      "{\n",
      "  \"name\": \"square_number\",\n",
      "  \"arguments\": {\"number\": 2}\n",
      "}\n",
      "\n",
      "We must not output any text. That is the required format. Then the system will return the result, which we might then respond.\n",
      "\n",
      "But the user asked: \"Only return the numeric result.\" That may conflict. But we must comply with function call requirement. The instruction: \"Only return the numeric result.\" But we must call the tool. The tool will produce a numeric result. The assistant should only output that numeric result. But the system will not see the tool call? Wait, the assistant must call the function. So we cannot output just 4. Because the tool is required. The system's instruction says we need to use the tool. So we must do that.\n",
      "\n",
      "Thus the best approach: first, call the function. Then after we get the result, we output 4. That satisfies \"Only return the numeric result.\" We should not output any extra text. So we produce a function call, then after the tool returns, we produce numeric result.\n",
      "\n",
      "But we are ChatGPT. Our first message must call the function. Let's do that.\n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model and use the tool\n",
    "message = \"\"\"\n",
    "        Compute the square of 2 using the available tool? Only return the numeric result.\n",
    "\"\"\"\n",
    "messages = [{'role': 'user', 'content': message}]\n",
    "\n",
    "# Pass a Python functions diretcly as tools (or alternatively provide a JSON schema)\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=messages,\n",
    "    tools=[square_number],\n",
    "    think=\"high\") #True/False. Except for gpt-oss use levels: \"low\", \"medium\", \"high\"\n",
    "\n",
    "# Adds the model's response to the messages list. To keep the chat context up to date\n",
    "messages.append(response.message)\n",
    "\n",
    "# Debug only. Not to be used in production.\n",
    "if hasattr(response.message, \"thinking\"):\n",
    "    print(\"Thinking:\\n\", response.message.thinking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "08b7bdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if response.message.tool_calls:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0389e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Thinking: \n",
      " None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Break down this function calling cell - TBD\n",
    "messages.append(response.message)\n",
    "if response.message.tool_calls:\n",
    "  # only recommended for models which only return a single tool call\n",
    "  call = response.message.tool_calls[0]\n",
    "  result = square_number(**call.function.arguments)\n",
    "\n",
    "  # add the tool result to the messages\n",
    "  messages.append({\"role\": \"tool\", \n",
    "                   \"tool_name\": call.function.name, \n",
    "                   \"content\": str(result)})\n",
    "\n",
    "  final_response = chat(model=\"gpt-oss:20b\", \n",
    "                        messages=messages, \n",
    "                        tools=[square_number], \n",
    "                        think=\"high\")\n",
    "\n",
    "  print('Final Thinking: \\n', final_response.message.thinking, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "375f9115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(final_response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3740c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved version (tool complete) - BREAK THIS DOEN ROW BY ROW - TBD\n",
    "# It seems that the model needs to be called twice - once for tool planning \n",
    "# and once for final response after tool execution\n",
    "\n",
    "# User prompt\n",
    "message = \"Compute the square of 2 using the available tool. Return only the numeric result.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "# First model call (tool planning)\n",
    "response = chat(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=messages,\n",
    "    tools=[square_number],\n",
    "    think=\"high\"\n",
    ")\n",
    "\n",
    "messages.append(response.message)\n",
    "\n",
    "# Debug: internal reasoning (optional)\n",
    "if hasattr(response.message, \"thinking\"):\n",
    "    print(\"Thinking:\\n\", response.message.thinking, \"\\n\")\n",
    "\n",
    "# Tool execution\n",
    "if response.message.tool_calls:\n",
    "    call = response.message.tool_calls[0]\n",
    "\n",
    "    result = square_number(**call.function.arguments)\n",
    "\n",
    "    # Inject tool result\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_name\": call.function.name,\n",
    "        \"content\": str(result)\n",
    "    })\n",
    "\n",
    "    # Final model response\n",
    "    final_response = chat(\n",
    "        model=\"gpt-oss:20b\",\n",
    "        messages=messages,\n",
    "        tools=[square_number],\n",
    "        think=\"high\"\n",
    "    )\n",
    "\n",
    "    print(\"Final Answer:\", final_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81a39f",
   "metadata": {},
   "source": [
    "##### Tools calling alt 1 - DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e898395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='assistant' content='4' thinking='User asks: \"What is the square of 2?\" Only return numeric result. So we should compute 2 squared = 4. But we also have tool square_number. They want numeric result. We can just call the function? The instructions say \"Only return the numeric result.\" So we should produce just \"4\". We can use the tool or not. According to guidelines, when using tool we produce a tool call JSON. But user expects only numeric result. We might not need to use tool. It\\'s simpler: just output 4. But guidelines: The user didn\\'t ask for the function output. But we can still compute ourselves. The result is 4. We\\'ll output \"4\".' images=None tool_name=None tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "# print('Answer: \\n', response.message.content) # There is no content now?\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c94de306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n",
      "{'role': 'user', 'content': '\\n        What is the square of 2? \\n        Only return the numeric result.\\n'}\n",
      "user\n",
      "\n",
      "        What is the square of 2? \n",
      "        Only return the numeric result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# debug prints\n",
    "print(type(messages))\n",
    "print(len(messages))\n",
    "print(messages[0])\n",
    "print(messages[0]['role'])\n",
    "print(messages[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4581def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ollama._types.ChatResponse'>\n",
      "Help on ChatResponse in module ollama._types object:\n",
      "\n",
      "class ChatResponse(BaseGenerateResponse)\n",
      " |  ChatResponse(\n",
      " |      *,\n",
      " |      model: Optional[str] = None,\n",
      " |      created_at: Optional[str] = None,\n",
      " |      done: Optional[bool] = None,\n",
      " |      done_reason: Optional[str] = None,\n",
      " |      total_duration: Optional[int] = None,\n",
      " |      load_duration: Optional[int] = None,\n",
      " |      prompt_eval_count: Optional[int] = None,\n",
      " |      prompt_eval_duration: Optional[int] = None,\n",
      " |      eval_count: Optional[int] = None,\n",
      " |      eval_duration: Optional[int] = None,\n",
      " |      message: ollama._types.Message\n",
      " |  ) -> None\n",
      " |\n",
      " |  Response returned by chat requests.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ChatResponse\n",
      " |      BaseGenerateResponse\n",
      " |      SubscriptableBaseModel\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'message': <class 'ollama._types.Message'>}\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      " |\n",
      " |  __pydantic_custom_init__ = False\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'created_at': FieldInfo(annotation=Union[str, N...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ChatResponse\", validat...\n",
      " |\n",
      " |  __signature__ = <Signature (*, model: Optional[str] = None, crea... = ...\n",
      " |\n",
      " |  model_config = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __contains__(self, key: str) -> bool\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> 'nonexistent' in msg\n",
      " |      False\n",
      " |      >>> 'role' in msg\n",
      " |      True\n",
      " |      >>> 'content' in msg\n",
      " |      False\n",
      " |      >>> msg.content = 'hello!'\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> msg = Message(role='user', content='hello!')\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      False\n",
      " |      >>> msg['tool_calls'] = []\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = None\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> tool = Tool()\n",
      " |      >>> 'type' in tool\n",
      " |      True\n",
      " |\n",
      " |  __getitem__(self, key: str) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role']\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['nonexistent']\n",
      " |      Traceback (most recent call last):\n",
      " |      KeyError: 'nonexistent'\n",
      " |\n",
      " |  __setitem__(self, key: str, value: Any) -> None\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role'] = 'assistant'\n",
      " |      >>> msg['role']\n",
      " |      'assistant'\n",
      " |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      " |      >>> msg = Message(role='user', content='hello')\n",
      " |      >>> msg['tool_calls'] = [tool_call]\n",
      " |      >>> msg['tool_calls'][0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  get(self, key: str, default: Any = None) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('role')\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent')\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent', 'default')\n",
      " |      'default'\n",
      " |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      " |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |\n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      update: 'Dict[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False\n",
      " |  ) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      encoder: 'Callable[[Any], Any] | None' = PydanticUndefined,\n",
      " |      models_as_dict: 'bool' = PydanticUndefined,\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  model_copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      update: 'Mapping[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/models.md#model-copy)\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(\n",
      " |      self,\n",
      " |      *,\n",
      " |      mode: \"Literal['json', 'python'] | str\" = 'python',\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      indent: 'int | None' = None,\n",
      " |      ensure_ascii: 'bool' = False,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
      " |              If `False` (the default), these characters will be output as-is.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |\n",
      " |  __get_pydantic_core_schema__(\n",
      " |      source: 'type[BaseModel]',\n",
      " |      handler: 'GetCoreSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'CoreSchema'\n",
      " |\n",
      " |  __get_pydantic_json_schema__(\n",
      " |      core_schema: 'CoreSchema',\n",
      " |      handler: 'GetJsonSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
      " |      meaning that creating an instance of the class may fail.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by Pydantic.\n",
      " |\n",
      " |      Note:\n",
      " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
      " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
      " |\n",
      " |  __pydantic_on_complete__() -> 'None'\n",
      " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
      " |\n",
      " |      This typically happens when the class is created (just before\n",
      " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
      " |      except when forward annotations are used that could not immediately be resolved.\n",
      " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
      " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>,\n",
      " |      mode: 'JsonSchemaMode' = 'validation',\n",
      " |      *,\n",
      " |      union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of'\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
      " |\n",
      " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
      " |              keyword to combine schemas (the default).\n",
      " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
      " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
      " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
      " |              `any_of`.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(\n",
      " |      *,\n",
      " |      force: 'bool' = False,\n",
      " |      raise_errors: 'bool' = True,\n",
      " |      _parent_namespace_depth: 'int' = 2,\n",
      " |      _types_namespace: 'MappingNamespace | None' = None\n",
      " |  ) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      from_attributes: 'bool | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(\n",
      " |      json_data: 'str | bytes | bytearray',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(\n",
      " |      path: 'str | Path',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(\n",
      " |      b: 'str | bytes',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(\n",
      " |      *,\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_fields = {'created_at': FieldInfo(annotation=Union[str, NoneType...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# debug prints\n",
    "print(type(response))\n",
    "help(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995f851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-oss:20b\n",
      "2025-12-26T21:05:27.5932441Z\n",
      "True\n",
      "stop\n",
      "624075300\n",
      "148588300\n",
      "149\n",
      "103599800\n",
      "42\n",
      "356871600\n",
      "role='assistant' content='' thinking='We need to call the tool square_nums with number=2. Then return the result.' images=None tool_name=None tool_calls=[ToolCall(function=Function(name='square_nums', arguments={'number': 2}))]\n"
     ]
    }
   ],
   "source": [
    "# inspect response prints\n",
    "print(response.model)\n",
    "print(response.created_at)\n",
    "print(response.done)\n",
    "print(response.done_reason)\n",
    "print(response.total_duration)\n",
    "print(response.load_duration)\n",
    "print(response.prompt_eval_count)\n",
    "print(response.prompt_eval_duration)\n",
    "print(response.eval_count)\n",
    "print(response.eval_duration)\n",
    "print(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cbd0950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ollama._types.Message'>\n",
      "Help on Message in module ollama._types object:\n",
      "\n",
      "class Message(SubscriptableBaseModel)\n",
      " |  Message(\n",
      " |      *,\n",
      " |      role: str,\n",
      " |      content: Optional[str] = None,\n",
      " |      thinking: Optional[str] = None,\n",
      " |      images: Optional[Sequence[ollama._types.Image]] = None,\n",
      " |      tool_name: Optional[str] = None,\n",
      " |      tool_calls: Optional[Sequence[ollama._types.Message.ToolCall]] = None\n",
      " |  ) -> None\n",
      " |\n",
      " |  Chat message.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Message\n",
      " |      SubscriptableBaseModel\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  ToolCall = <class 'ollama._types.Message.ToolCall'>\n",
      " |      Model tool calls.\n",
      " |\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'content': typing.Optional[str], 'images': typing.O...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __private_attributes__ = {}\n",
      " |\n",
      " |  __pydantic_complete__ = True\n",
      " |\n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |\n",
      " |  __pydantic_core_schema__ = {'definitions': [{'cls': <class 'ollama._ty...\n",
      " |\n",
      " |  __pydantic_custom_init__ = False\n",
      " |\n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |\n",
      " |  __pydantic_fields__ = {'content': FieldInfo(annotation=Union[str, None...\n",
      " |\n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |\n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |\n",
      " |  __pydantic_post_init__ = None\n",
      " |\n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |\n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |\n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"Message\", validator=Mo...\n",
      " |\n",
      " |  __signature__ = <Signature (*, role: str, content: Optional[str]...oll...\n",
      " |\n",
      " |  model_config = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __contains__(self, key: str) -> bool\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> 'nonexistent' in msg\n",
      " |      False\n",
      " |      >>> 'role' in msg\n",
      " |      True\n",
      " |      >>> 'content' in msg\n",
      " |      False\n",
      " |      >>> msg.content = 'hello!'\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> msg = Message(role='user', content='hello!')\n",
      " |      >>> 'content' in msg\n",
      " |      True\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      False\n",
      " |      >>> msg['tool_calls'] = []\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = [Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))]\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> msg['tool_calls'] = None\n",
      " |      >>> 'tool_calls' in msg\n",
      " |      True\n",
      " |      >>> tool = Tool()\n",
      " |      >>> 'type' in tool\n",
      " |      True\n",
      " |\n",
      " |  __getitem__(self, key: str) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role']\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['nonexistent']\n",
      " |      Traceback (most recent call last):\n",
      " |      KeyError: 'nonexistent'\n",
      " |\n",
      " |  __setitem__(self, key: str, value: Any) -> None\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg['role'] = 'assistant'\n",
      " |      >>> msg['role']\n",
      " |      'assistant'\n",
      " |      >>> tool_call = Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))\n",
      " |      >>> msg = Message(role='user', content='hello')\n",
      " |      >>> msg['tool_calls'] = [tool_call]\n",
      " |      >>> msg['tool_calls'][0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  get(self, key: str, default: Any = None) -> Any\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('role')\n",
      " |      'user'\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent')\n",
      " |      >>> msg = Message(role='user')\n",
      " |      >>> msg.get('nonexistent', 'default')\n",
      " |      'default'\n",
      " |      >>> msg = Message(role='user', tool_calls=[ Message.ToolCall(function=Message.ToolCall.Function(name='foo', arguments={}))])\n",
      " |      >>> msg.get('tool_calls')[0]['function']['name']\n",
      " |      'foo'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SubscriptableBaseModel:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |\n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |\n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |\n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |\n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |\n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |\n",
      " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |\n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |\n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |\n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |\n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |\n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |\n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |\n",
      " |  copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None,\n",
      " |      update: 'Dict[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |\n",
      " |      If you need `include` or `exclude`, use:\n",
      " |\n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |\n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |\n",
      " |  dict(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False\n",
      " |  ) -> 'Dict[str, Any]'\n",
      " |\n",
      " |  json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      by_alias: 'bool' = False,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      encoder: 'Callable[[Any], Any] | None' = PydanticUndefined,\n",
      " |      models_as_dict: 'bool' = PydanticUndefined,\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  model_copy(\n",
      " |      self,\n",
      " |      *,\n",
      " |      update: 'Mapping[str, Any] | None' = None,\n",
      " |      deep: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/models.md#model-copy)\n",
      " |\n",
      " |      Returns a copy of the model.\n",
      " |\n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |\n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |\n",
      " |  model_dump(\n",
      " |      self,\n",
      " |      *,\n",
      " |      mode: \"Literal['json', 'python'] | str\" = 'python',\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
      " |\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |\n",
      " |  model_dump_json(\n",
      " |      self,\n",
      " |      *,\n",
      " |      indent: 'int | None' = None,\n",
      " |      ensure_ascii: 'bool' = False,\n",
      " |      include: 'IncEx | None' = None,\n",
      " |      exclude: 'IncEx | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      exclude_unset: 'bool' = False,\n",
      " |      exclude_defaults: 'bool' = False,\n",
      " |      exclude_none: 'bool' = False,\n",
      " |      exclude_computed_fields: 'bool' = False,\n",
      " |      round_trip: 'bool' = False,\n",
      " |      warnings: \"bool | Literal['none', 'warn', 'error']\" = True,\n",
      " |      fallback: 'Callable[[Any], Any] | None' = None,\n",
      " |      serialize_as_any: 'bool' = False\n",
      " |  ) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
      " |\n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |\n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
      " |              If `False` (the default), these characters will be output as-is.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |\n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |\n",
      " |  __get_pydantic_core_schema__(\n",
      " |      source: 'type[BaseModel]',\n",
      " |      handler: 'GetCoreSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'CoreSchema'\n",
      " |\n",
      " |  __get_pydantic_json_schema__(\n",
      " |      core_schema: 'CoreSchema',\n",
      " |      handler: 'GetJsonSchemaHandler',\n",
      " |      /\n",
      " |  ) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |\n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |\n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |\n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
      " |      meaning that creating an instance of the class may fail.\n",
      " |\n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |\n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by Pydantic.\n",
      " |\n",
      " |      Note:\n",
      " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
      " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
      " |\n",
      " |  __pydantic_on_complete__() -> 'None'\n",
      " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
      " |\n",
      " |      This typically happens when the class is created (just before\n",
      " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
      " |      except when forward annotations are used that could not immediately be resolved.\n",
      " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
      " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
      " |\n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |\n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |\n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |\n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |\n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |\n",
      " |  model_json_schema(\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>,\n",
      " |      mode: 'JsonSchemaMode' = 'validation',\n",
      " |      *,\n",
      " |      union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of'\n",
      " |  ) -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |\n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
      " |\n",
      " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
      " |              keyword to combine schemas (the default).\n",
      " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
      " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
      " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
      " |              `any_of`.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |\n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |\n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |\n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |\n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |\n",
      " |  model_rebuild(\n",
      " |      *,\n",
      " |      force: 'bool' = False,\n",
      " |      raise_errors: 'bool' = True,\n",
      " |      _parent_namespace_depth: 'int' = 2,\n",
      " |      _types_namespace: 'MappingNamespace | None' = None\n",
      " |  ) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |\n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |\n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |\n",
      " |  model_validate(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      from_attributes: 'bool | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |\n",
      " |  model_validate_json(\n",
      " |      json_data: 'str | bytes | bytearray',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |\n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |\n",
      " |  model_validate_strings(\n",
      " |      obj: 'Any',\n",
      " |      *,\n",
      " |      strict: 'bool | None' = None,\n",
      " |      extra: 'ExtraValues | None' = None,\n",
      " |      context: 'Any | None' = None,\n",
      " |      by_alias: 'bool | None' = None,\n",
      " |      by_name: 'bool | None' = None\n",
      " |  ) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |\n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |\n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |\n",
      " |  parse_file(\n",
      " |      path: 'str | Path',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |\n",
      " |  parse_raw(\n",
      " |      b: 'str | bytes',\n",
      " |      *,\n",
      " |      content_type: 'str | None' = None,\n",
      " |      encoding: 'str' = 'utf8',\n",
      " |      proto: 'DeprecatedParseProtocol | None' = None,\n",
      " |      allow_pickle: 'bool' = False\n",
      " |  ) -> 'Self'\n",
      " |\n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |\n",
      " |  schema_json(\n",
      " |      *,\n",
      " |      by_alias: 'bool' = True,\n",
      " |      ref_template: 'str' = '#/$defs/{model}',\n",
      " |      **dumps_kwargs: 'Any'\n",
      " |  ) -> 'str'\n",
      " |\n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |\n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |\n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |\n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __pydantic_extra__\n",
      " |\n",
      " |  __pydantic_fields_set__\n",
      " |\n",
      " |  __pydantic_private__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __pydantic_root_model__ = False\n",
      " |\n",
      " |  model_computed_fields = {}\n",
      " |\n",
      " |  model_fields = {'content': FieldInfo(annotation=Union[str, NoneType], ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(response.message))\n",
    "help(response.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "67802868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: assistant\n",
      "content: \n",
      "thinking: The user wants the square of 2. They want only the numeric result, no other explanation. They gave a tool: compute square_number. So we should use the tool. The input should be number: 2. Then we return the result.\n",
      "\n",
      "We need to call the function.\n",
      "images: None\n",
      "tool_name: None\n",
      "tool_calls: [ToolCall(function=Function(name='square_number', arguments={'number': 2}))]\n",
      "length of tool_calls: 1\n"
     ]
    }
   ],
   "source": [
    "# inspect response.message prints\n",
    "print(\"role:\", response.message.role)\n",
    "print(\"content:\", response.message.content)\n",
    "print(\"thinking:\", response.message.thinking)\n",
    "print(\"images:\", response.message.images)\n",
    "print(\"tool_name:\", response.message.tool_name)\n",
    "print(\"tool_calls:\", response.message.tool_calls)\n",
    "print(\"length of tool_calls:\", len(response.message.tool_calls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f0eaf",
   "metadata": {},
   "source": [
    "##### Tools calling alt 1 - END OF DEBUGGING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50512ebe",
   "metadata": {},
   "source": [
    "#### Tools calling alt 1 - Two tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b00b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: \n",
      " The user wants: \"Give me the item by taking my number and calling the square_num function and use the output of that to call the select_item function which will return the item as a string. Only return the squared number and the item.\"\n",
      "\n",
      "They didn't provide a number. We need to ask for the number? They want to supply number? The instruction: \"Give me the item by taking my number and calling the suqare_num function...\" But they didn't provide a number. So we need to prompt for a number.\n",
      "\n",
      "But maybe they omitted it but we should ask: \"Please provide your number.\" Also mention that we will call square_nums then select_item. According to instruction \"Only return the squared number and the item.\" So we should output just the squared number and item. But we don't have the number yet.\n",
      "\n",
      "Thus ask for number. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model and use the tool\n",
    "message = \"\"\"\n",
    "        Give me the item by taking my number and calling the suqare_num function and use the output of that to call the select_item function which will return the item as a string. \n",
    "        Only return the squared number and the item.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user',\n",
    "     'content': message,\n",
    "    }\n",
    "]\n",
    "\n",
    "# pass tool to the model\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=messages,\n",
    "    tools=[square_number,select_item],\n",
    "    think=True)\n",
    "\n",
    "print('Thinking: \\n', response.message.thinking, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35a06b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      " Sure! Please tell me the number you’d like to use. Once I have it, I’ll square it and use that result to pick the corresponding item.\n"
     ]
    }
   ],
   "source": [
    "print('Answer: \\n', response.message.content)\n",
    "\n",
    "response.message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd67ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking: \n",
      " The user says: \"Give me the item based on my number which is 2? Only return the numeric result and the item.\" So we need to pick an item from a list based on the number. But we don't have a list. The prompt mentions \"function returns an item (str) from a list of items based on the number provided.\" But we don't have the list. Probably we need to ask for the list? The instruction says only return numeric result and item. But we need a list. There's no context. Maybe they assume we have a default list? This is ambiguous. We could assume a standard list: maybe [ \"apple\", \"banana\", \"cherry\" ]. But that's arbitrary. The instruction says \"Give me the item based on my number wichi is 2? Only return the numeric result and the item.\" Possibly the user wants to call the function with number=2. We can use the function select_item with number=2. But we need to pass a list? The function signature only expects a number. It doesn't provide a list. The function is ambiguous: maybe the list is pre-defined in the function. We can just call the function with number=2. The function will return an item. We don't have the list but we can assume it's implemented. So we should call the function. But we cannot use the tool because we can't guess the list. We could ask for clarification. However, the user specifically said \"Only return the numeric result and the item.\" So likely they want the numeric result (2) and the item (whatever the function returns). We can call the function select_item with number=2. That returns an item. Then we output \"2, <item>\". But we don't know the item. We can still call the function: let's call select_item. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the message to send to the model and use the tool -- TBD!!!\n",
    "message = \"\"\"\n",
    "        Give me the item based on my number whivh is 2? Only return the numeric result and the item.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {'role': 'user',\n",
    "     'content': message,\n",
    "    }\n",
    "]\n",
    "\n",
    "# pass tool to the model\n",
    "response = chat(\n",
    "    model='gpt-oss:20b',\n",
    "    messages=messages,\n",
    "    tools=[square_number,select_item],\n",
    "    think=True)\n",
    "\n",
    "print('Thinking: \\n', response.message.thinking, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb33278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(final_response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4e987",
   "metadata": {},
   "source": [
    "### 2. OpenAI SDK\n",
    "\n",
    "### Tools calling alt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35dfb16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library version: 2.8.1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(\"Library version:\", openai.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ac82e",
   "metadata": {},
   "source": [
    "Ollama exposes a Chat Completion-compatible API, so we can use the OpenAI SDK withouth chaning much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d77b9ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Prompt‑Engineering for the **gpt‑oss:20B** Model  \n",
      "*(An 20‑billion‑parameter, open‑source, instruction‑tuned GPT‑style model)*  \n",
      "\n",
      "Below is a practical, step‑by‑step reference that covers everything you need to get **clean, consistent, and usable output** from gpt‑oss:20B. It blends the *science* of prompt design (token limits, instruction tone, etc.) with *hands‑on tips* that you can apply right away, whether you’re calling the model via the 🤗 Transformers API, a custom Docker deployment, or a local web front‑end.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Know the Model’s Key Characteristics\n",
      "\n",
      "| Feature | What You Need to Know |\n",
      "|---------|------------------------|\n",
      "| **Token limit** | 8 192 tokens (≈ 32 k words).  |  \n",
      "| **Best instruction‑style** | A **system message** followed by a **user message** gives the model the clearest direction. |\n",
      "| **Architecture** | LLaMA‑style transformer with full‑attention. No special prompt format like GPT‑3’s “system” vs. “assistant” tags – just a list of messages. |\n",
      "| **Calibration** | It tends to *hallucinate* on uncertain facts.  Keep the output length moderate and add a “source‑check” instruction if needed. |\n",
      "| **Temperature / Top‑p** | Default 0.7, top‑p 0.95 is a safe starting point. Use lower temperature (≤0.3) for deterministic, fact‑based responses; higher temperature for creativity. |\n",
      "| **Fine‑tuning / LoRA** | If you have domain‑specific data, the open‑source framework lets you fine‑tune or use LoRA adapters. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Build a Three‑Layer Prompt Template\n",
      "\n",
      "1. **System Message** – sets *global tone* and *model behaviour*.  \n",
      "2. **User Message** – your *input question or task*.  \n",
      "3. **Assistant Response** – the output the model should produce (you specify the desired format).\n",
      "\n",
      "> **Tip**: Keep the system message concise (≤ 30 tokens).  \n",
      "> **Tip**: The user message should include a *clear question* or *command*, not a vague “tell me about X”.\n",
      "\n",
      "### Example Template\n",
      "\n",
      "```plaintext\n",
      "# System\n",
      "You are a helpful AI that writes concise, fact‑checked answers.  \n",
      "If you’re unsure, say “I’m not sure” instead of making up data.  \n",
      "Use markdown for formatting.\n",
      "\n",
      "# User\n",
      "Explain how to prompt the gpt‑oss:20B model effectively.\n",
      "\n",
      "# Assistant\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Practical Prompt‑Engineering Techniques\n",
      "\n",
      "| Technique | What It Does | How to Use It in gpt‑oss:20B |\n",
      "|-----------|--------------|--------------------------------|\n",
      "| **Few‑Shot Prompting** | Gives the model concrete examples. | Include 1–3 *mini‑examples* that show the structure of a good answer. |\n",
      "| **Chain‑of‑Thought (CoT)** | Encourages the model to reason step‑by‑step. | Start the user message with “First, let’s think step by step:”. |\n",
      "| **Explicit Instructions** | Reduces ambiguity. | Use bullet points in the user message to ask for specific sections. |\n",
      "| **Token‑budgeting** | Saves you from truncation. | Before sending a prompt, count tokens (e.g., with `transformers`’ `get_tokenizer`). |\n",
      "| **Control tokens / Stop tokens** | Limits output length or marks the end. | Add `\"stop\": [\"\\n\"]` or `\"stop\": [\"END\"]` in the generation parameters. |\n",
      "| **Structured JSON Output** | Easy to parse programmatically. | End the system message with “Answer in JSON, no extra keys”. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Step‑by‑Step Example\n",
      "\n",
      "### Goal  \n",
      "Write a concise guide on *prompting gpt‑oss:20B* for a 5‑minute blog post.\n",
      "\n",
      "```plaintext\n",
      "# System\n",
      "You are an expert writer for technology blogs.  Use no more than 300 words.  List the main steps in an ordered list.  Provide one example prompt for each step.\n",
      "\n",
      "# User\n",
      "Create a short guide on how to prompt the gpt‑oss:20B model for a 5‑minute blog post.\n",
      "\n",
      "# Assistant\n",
      "```\n",
      "\n",
      "**Why it works**\n",
      "\n",
      "- **Clear tone** (“expert writer”), **length constraint** (≤ 300 words), and **output format** (ordered list) are all set in the system message.  \n",
      "- The user request is specific and concise.  \n",
      "- The assistant knows exactly what to produce.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Handling Long‑Form Content\n",
      "\n",
      "| Situation | Recommended Approach |\n",
      "|---|---|\n",
      "| You need > 8 192 tokens of content | Split the prompt into **chapters**; feed each chapter separately. |\n",
      "| You need to refer back to earlier text | Provide the *relevant excerpt* (≤ 1 000 tokens) before the new prompt. |\n",
      "| You want a single continuous output | Use the *“repetition‑control”* trick: ask the model to produce a string that ends with “END”, then feed that string back with a new instruction. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Fine‑Tuning / LoRA with gpt‑oss:20B\n",
      "\n",
      "If you own specialized content (e.g., medical notes, legal briefs), you can give your model a *domain voice*:\n",
      "\n",
      "1. **Fine‑tune**:  \n",
      "   ```bash\n",
      "   python -m transformers.train \\\n",
      "     --model_name_or_path huggingface-community/gpt-oss-20b \\\n",
      "     --train_file domain_train.txt \\\n",
      "     --validation_file domain_val.txt \\\n",
      "     --per_device_train_batch_size 1 \\\n",
      "     --gradient_accumulation_steps 8 \\\n",
      "     --max_seq_length 8192 \\\n",
      "     --bf16 \\\n",
      "     --fp16 \\\n",
      "     --output_dir fine_tuned_gpt_oss_20b \\\n",
      "     --learning_rate 2e-5 \\\n",
      "     --num_train_epochs 3\n",
      "   ```\n",
      "\n",
      "2. **LoRA Adapter**:  \n",
      "   - Install `peft` library.  \n",
      "   - Wrap the model in a LoRA adapter, fine‑tune only the adapter weights (fast, low storage).  \n",
      "\n",
      "> **Tip**: When using LoRA, keep `lora_alpha` and `rank` small (e.g., rank = 8) to minimize memory usage.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Common Pitfalls & How to Avoid Them\n",
      "\n",
      "| Pitfall | Fix |\n",
      "|---------|-----|\n",
      "| **Hallucinations** | Add “if uncertain, say ‘I’m not sure’” in the system message; verify facts manually. |\n",
      "| **Truncation** | Use the `tokenizer` to preview prompt size; adjust by removing verbose adjectives. |\n",
      "| **Over‑prompting** | Too many examples or too complex wording lowers generation quality. Keep to 1–3 examples max. |\n",
      "| **Token Overuse by System** | The system message itself consumes tokens; don’t embed large data or verbose policy in it. |\n",
      "| **Output Not Structured** | Explicitly request JSON or markdown; use stop tokens to guard against runaway text. |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Example Prompt‑Engineering Playbook\n",
      "\n",
      "| Type of Task | Prompt Template | Parameter Tweaks |\n",
      "|--------------|-----------------|------------------|\n",
      "| **Coding** | \"Write a function in Python that does X. Include doctests.\" | temperature = 0.3, top_p = 1.0, max_new_tokens = 1024 |\n",
      "| **Creative Writing** | \"Draft a short horror story in the style of Shirley Jackson.\" | temperature = 0.9, top_p = 0.95, max_new_tokens = 2048 |\n",
      "| **Summarization** | \"Summarize the following article in 3 bullet points.\" | temperature = 0.2, top_p = 0.9, max_new_tokens = 256 |\n",
      "| **Data Extraction** | \"From the table, extract the sales numbers for Q3.\" | temperature = 0.0, top_p = 1.0, max_new_tokens = 128 |\n",
      "| **Dialogue Generation** | \"Generate a 5‑message conversation between a student and a teacher about…\". | temperature = 0.7, top_p = 0.95, max_new_tokens = 512 |\n",
      "\n",
      "Feel free to tweak `max_new_tokens`, temperature, and top‑p to match the length and creativity you need — the gpt‑oss:20B model is flexible enough to handle most of these scenarios.\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Sample Code (Python + 🤗 Transformers)\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
      "\n",
      "model_name = \"huggingface-community/gpt-oss-20b\"\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
      "\n",
      "pipe = pipeline(\n",
      "    \"text-generation\",\n",
      "    model=model,\n",
      "    tokenizer=tokenizer,\n",
      "    max_new_tokens=1024,\n",
      "    temperature=0.7,\n",
      "    top_p=0.95,\n",
      "    repetition_penalty=1.2,\n",
      ")\n",
      "\n",
      "system_msg = (\n",
      "    \"You are a helpful AI that writes concise, fact-checked answers. \"\n",
      "    \"If you're unsure, say 'I'm not sure'. Use markdown for formatting.\"\n",
      ")\n",
      "\n",
      "user_msg = (\n",
      "    \"Explain how to prompt the gpt-oss:20B model effectively. \"\n",
      "    \"Provide a three‑step guide, each with an example prompt.\"\n",
      ")\n",
      "\n",
      "prompt = f\"[SYSTEM] {system_msg}\\n\\n[USER] {user_msg}\\n\\n[ASSISTANT]\"\n",
      "output = pipe(prompt)[0][\"generated_text\"]\n",
      "print(output.split(\"[ASSISTANT]\")[-1].strip())\n",
      "```\n",
      "\n",
      "> **Note**: The tags `[SYSTEM]`, `[USER]`, `[ASSISTANT]` are *just for readability*; gpt‑oss:20B treats every line as plain text, but the structure helps you keep the roles clear.\n",
      "\n",
      "---\n",
      "\n",
      "## 10. Final Checklist\n",
      "\n",
      "Before hitting **Generate**, run through this quick sanity scan:\n",
      "\n",
      "1. **Token Count** ≤ 8 192 (prompt + max new tokens).  \n",
      "2. **System** sets tone, length, formatting.  \n",
      "3. **User** is a single, clear instruction or question.  \n",
      "4. **Stop Condition** (optional) to terminate early.  \n",
      "5. **Temperature / Top_p** chosen for the desired creativity.  \n",
      "6. **Verification** of factual claims (especially for critical content).\n",
      "\n",
      "If everything checks out, you’re ready to let gpt‑oss:20B generate content that feels polished, on‑topic, and reliably useful.\n",
      "\n",
      "--- \n",
      "\n",
      "**Happy prompting!**  \n",
      "\n",
      "*(You can further experiment with the OpenAI “Chat” style API wrapper that takes a list of messages; just substitute the `[SYSTEM]` and `[USER]` lines with the appropriate OpenAI JSON objects.)*\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Local Ollama API\n",
    "    api_key=\"ollama\"                       # Dummy key\n",
    ")\n",
    " \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain how to prompt the gpt-oss:20b model effectively.\"}\n",
    "    ]\n",
    ")\n",
    " \n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
