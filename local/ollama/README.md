# Ollama  

This folder contains models that are downloaded and locally executed using Ollama local LLM runtime. Ollama runs as a background server and models are loaded on demand. It allows model interaction using: CLI, REST API (default: `http://localhost:11434Â´) or Python HTTP calls.  
  
## Models
- [x] gpt_oss  
- [ ] *nemotron* - TBD  
- [ ] *devstral* -TBD  
